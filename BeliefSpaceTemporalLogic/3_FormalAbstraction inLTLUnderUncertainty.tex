\documentclass{ifacconf}
\usepackage{standalone}
\usepackage{times}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}      % include this line if your document contains figures
\usepackage{natbib}        % required for bibliography
%===========================================
\input{TemplateFiles/inc}
\input{TemplateFiles/commands.tex} 
\pdfinfo{
   /Author (S.Haesaert et al.)
   /Title  (Formal abstraction of POMDPs for Distribution LTL)
   /CreationDate (D:20101201120000)
   /Subject (Formal abstraction)
   /Keywords (abstraction;POMDP)
}

% Table caption wrangling
\usepackage{etoolbox}
 

\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{tikz}


\allowdisplaybreaks[1]
%% commenting
\newcommand{\red}[1]{{\color{red} #1}}
\renewcommand{\axx}[1]{{\color{orange} Ali: #1}}

\newcommand{\new}[1]{{\color{blue}#1}}
\newcommand{\ind}{\mathbf{1}}


\begin{document}


\begin{frontmatter}

\title{\huge Refinement-based temporal logic control of partially observable Markov decision processes }
%\thanks[footnoteinfo]{Sponsor and financial support acknowledgment
%goes here. Paper titles should be written in uppercase and lowercase
%letters, not all uppercase.}

\author[cal]{S. Haesaert} 
\author[cal]{P. Nilsson} 
\author[mit]{C.I. Vasile}
\author[jpl]{R. Thakker}
\author[jpl]{A. Agha}
\author[cal]{A.D.  Ames}
\author[cal]{R. M. Murray}



\address[cal]{California Institute of Technology, 
   Pasadena, CA 91125 USA} % (e-mail: \{haesaert,pettni,ames,murray\}@caltech ).}
\address[mit]{Massachusetts Institute of Technology, 
   Cambridge, MA 02139 USA}% (e-mail:  cvasile@mit.edu)}
\address[jpl]{Jet Propulsion Laboratory, 
   Pasadena, CA 91109 USA}% (e-mail: rohan.a.thakker@jpl.nasa.gov)} 
\maketitle
\begin{abstract}
The stochastic evolution of the partially observable Markov decision process (POMDP) can be modeled by transitions in an associated belief Markov decision process.
In this work, we consider the synthesis of controllers guaranteeing  specifications given in linear temporal logic on these belief models.
For this, we use an approximate  abstraction %of the belief space model
 suitable for the correct-by-construction control synthesis. By designing a control policy over the abstract model and refining it back to the original model, the computational issues involved with control synthesis on the original model can be circumvented. 
We leverage the notion of approximate stochastic simulation to quantify the deviation of the approximate models.  % The accuracy is expressed by the deviations in transition probability and by increasing non-determinism in the labeling of the Markov decision processes.
By compensating {\it a priori} for these deviations in the control synthesis for the abstract model, the guarantees are preserved for the refined control policy.
\end{abstract}
\begin{keyword} Belief space models,
correct-by-construction controller synthesis, Markov decision processes, partially observable
\end{keyword}

\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\section{Introduction }\label{subsec:intro}
\red{[Start with motivating the big picture (maybe with a picture)]}
The specification and design of controllers for navigation problems has  been shown to be well expressible via temporal logic specifications \citep{Murray2009}.      To represent the low-level control and decision making under motion and sensing uncertainty common in navigation problems, we model the system
    in its most generic and principled form as a Partially-Observable Markov Decision Process
    (POMDP)~\citep{Kaelbling98,Smallwood73}.
As in \citep{JonesDTL2013}, we specify the temporal logic properties with respect to their likelihood or uncertainty over the belief space of a POMDP.
In this work, we newly  leverage approximate stochastic simulation relations in the synthesis of  the control policy,  
 
 
 %Unlike the available work \citep{Vasile2016,JonesDTL2013}, we will synthesize control policies formally with guarantees. \axx{the last sentence need to be updated. let's talk about it in the meeting.}

%We define the belief space models of these POMDPs  to design controllers is commonly used for navigation problems.   

Given a specification $\psi$  written in linear temporal logic with propositions over the belief state of a POMDP, we are interested 
in the design of a policy $\pol$ such that  $\psi$ is satisfied with probability at least $p$.
%\[\po^\pol_\init(\MB\vDash \psi)\geq p.\]
%
In this paper, we approach the synthesis problem as follows. 
Firstly,  for a given belief MDP $\MB$ a finite state abstraction  $\tilde \MB$ is computed.
Secondly,  we compute a labeling-based approximate stochastic  simulation relation between the abstract belief space model  $\tilde{\MB}$ and the concrete model $\MB$.
 Then,  a $\delta$-robust policy for the abstract $\tilde{\MB}$ is computed  together with a stationary value function for the associated stochastic optimal control problem.
  Finally, the policy for the concrete belief space model is computed  as its refinement.





\subsubsection{Literature.}For probabilistic temporal logic properties  over finite state Markov decision processes %expressed in PCTL and CSL 
there exist several tools for policy synthesis and verification such as PRISM \citep{KNP11} and  Storm \citep{dehnert2017storm}. 
For Markov decision processes over uncountable state spaces,  the characterization of properties cannot in general be
attained analytically \citep{Abate1}, so an alternative is to approximate these models by simpler
processes that are prone to be mathematically analyzed or algorithmically verified
such as finite-state MDPs \citep{soudjani2015faust},   or deterministic transition systems \citep{Zamani2014}.	
 In \citep{haesaert2017verification}, an approximate stochastic simulation relation has been introduced that allows for the use of  formal abstractions for the correct-by-construction control synthesis with respect to probabilistic linear time temporal logic properties \citep{tech_report_TACAS}. 
 
 
Without full-state observations, the formal synthesis of controllers over hidden state models
defines a more challenging problem.
 %Within the formal methods,  the synthesis of controllers over models without full-state observations is a more difficult problem. 
 For finite state POMDP there are results on the verification and policy synthesis  for PCTL properties \citep{Norman2017, Chatterjee2014}.
%\subsubsection{Technical math results for POMDPs.}
%The reduction of Markov decision problems with incomplete information to problems with complete information has been tackled in
%\cite{yushkevich_reduction_1976,rhenius_incomplete_1974}, with respect to Borel states and Borel actions. It is also extensively discussed in \citep[Chapter 11]{bertsekas2004stochastic}.
%In recent work of \cite{feinberg2014optimality}  and \cite{feinberg2016partially} the optimality conditions for POMDP problems solved via \red{COMDPs} are analyzed in more detail. 
%In \cite{saldi2017finite}, conditions for the convergence of solutions of finite abstractions of COMPDs to the optimal solutions of POMDPs are analyzed. 
%Most of the work (including the references given above) has been developed for additive cost functions or terminal cost functions defined via cost functions associated to the model. 
%
Results for specifications defined on the continuous states of a POMDP are rather preliminary and have been focused mainly on reachability and safety problems. 
In  \citep{ding2013optimal} and \citep{LESSER20141989}, reachability and safety  are defined with respect to the hidden state of the POMDP.
%
In \citep{ding2013optimal}, the optimal control of partially observable systems over safety specifications is analyzed.  The control synthesis solving the reachability problem over a  partially observable stochastic system is solved
in \citep{LESSER20141989}. %, the analysis of partially observable systems with as objective reachability is analyzed. \axx{grammar issue in the last sentence.}
%The reachability/safety problem can be formulated with a multiplicative cost function or by using a terminal cost function. In the latter case, the terminal function specifies fully the reachability/safety problem over an extended state space.
%The state extensions model the satisfaction of failing of reachability properties.

% Unlike the available work \citep{Vasile2016,JonesDTL2013}, we will synthesize control policies formally with guarantees. \axx{the last sentence need to be updated. let's talk about it in the meeting.}
 
In this paper, the properties of interest are defined directly on the belief space (i.e., the space of probability distributions - belief - over all possible states) as in \citep{Vasile2016,JonesDTL2013}. The properties over belief space express requirements on the estimation quality or uncertainty. 
We newly solve this type of synthesis problem in a correct-by-construction fashion.  For this we leverage the results in \citep{haesaert2017verification, tech_report_TACAS}, as such we can precede  discretization %to a finite abstraction
with model order reductions and simplifications.  Since we work over belief MDPs, this is crucial.  Further, and new in this paper, we also define an approximate stochastic simulation relation via non-determinism in the labeling, and we newly refine the control policy via the abstract value function.

% In contrast to the work in \citep{JonesDTL2013}, and  \citep{Vasile2016},  the  controllers are synthesized in a correct-by-construction fashion by leveraging formal similarity relations.
 
  
  %  STRUCTURE
 In the next section, the POMDPs and their belief models are first introduced together with the temporal logic used to define properties of interest. 
 Afterwards, we define the verification and control synthesis  problem for temporal logic properties over belief MDPs. 
 In Section \ref{sec:refinement}, the synthesis problem is solved robustly by newly using labeling-based stochastic simulation relation. In Section \ref{sec:case}, an illustrative case study is given. 
%Finally in the last section, the conclusions are given and the required future work to enable solving real-sized problems is discussed.
 
\section{POMDPs and temporal logic specifications.}

In this section, we give the definition of a Partially Observable Markov Decision Process (POMDP). Then we define a (propositional) linear temporal logic (LTL)  equivalent to the formulation of distribution temporal logic as originally given in \citep{JonesDTL2013} for POMDPs. 

%
%\subsection{Notation}
%
%
%We use the following notation for the mappings $\rel(\tilde A):=\{y: x\rel y,\  x\in \tilde A\}$ and $\rel^{-1}(\tilde B):=\{x: x\rel y,\  y\in \tilde B\}$  for $\tilde A\subseteq A$ and $\tilde B\subseteq B$.
%    Let $\Sigma$ be a finite set. The cardinality,
%    power set, Kleene- and $\omega$-closures
%    of $\Sigma$ are denoted by $|\Sigma|$,
%    $2^{\Sigma}$, $\Sigma^*$ and $\Sigma^\omega$,
%    respectively.    
%    Each member of $\Sigma^*$ and $\Sigma^\omega$ is referred to as ``word" or "sequence". 
%    
%   
%    $A \subseteq \BB{R}^n$ and $B \subseteq \BB{R}^m$,
%    $n, m \geq 0$, we denote by $\CA{M}(A, B)$ the set of
%    functions with domain $A$ and co-domain $B$, where $A$ has positive measure with
%    respect to the Lebesgue measure of $\BB{R}^n$.
%    
%    
%    The set of all positive semi-definite matrices of size
%    $n \times n$, $n \geq 1$, is denoted by $\Symb^n$.    The $m \times n$ zero matrix and the $n \times n$ identity matrix are denoted by $\BF{0}_{m, n}$ and $\BF{I}_n$, respectively.
   
%    The supremum and Euclidean norms are denoted by     $\norminf{\cdot}$ and $\normeucl{\cdot}$, respectively.     For a given set $\Z$ a metric or distance function $\mathbf d_\Z$ is a function $\mathbf{d}_\Z: \Z\times \Z\rightarrow \mathbb R_{\ge 0}$  satisfying the following conditions:  $\forall y_1,y_2,y_3\in\Z$: $\mathbf d_\Z(y_1,y_2)=0$ iff $y_1=y_2$;  $\mathbf d_\Z(y_1,y_2)=\mathbf d_{\Z}(y_2,y_1)$;  and $\mathbf d_\Z(y_1,y_3)\leq \mathbf d_\Z(y_1,y_2) +\mathbf d_\Z(y_2,y_3)$. 
 %


\subsection{Partially Observable Markov  Decision Processes}
For metric space $\Y$, we denote with  $\borel{\Y}$ its Borel $\sigma$-field, i.e., the  
collection of all sets that can be formed from countable unions and intersections of open sets in $\Y$.
We refer to  $(\Y,\borel{\Y})$ as a Borel measurable space and we denote with $\mathcal P(\Y)$ the set of probability measures \new{(or equivalently, probability distributions )} on $(\Y,\borel{\Y})$.
Together with the measurable space $(\Y,\borel{\Y})$,  a probability measure $\po$ defines the probability space, denoted by $(\Y,\mathcal{B}(\Y),\po)$ and has realizations  $s{\,\sim\,}\po$.     Given a probability measure, the corresponding expectation operator is denoted as  $\Ex[\cdot]$.
In this work,  we only consider Polish spaces, which are complete separable metric spaces; see \citep{bogachev2007measure}. 



    
Consider a Markov decision process \citep{bertsekas2004stochastic,mt1993,hll1996}, defined as follows.%
\begin{definition}[Markov decision process (MDP)]\label{def:MDP} \mbox{ }\\
  A discrete-time MDP is a tuple $\MDP = (\X, \init, \tr, \A)$ with
  \begin{itemize}
    \item $\X$,  a state space with states $x\in\X$; % as its elements;
    \item $\init$, the initial probability distribution \marginpar{\tiny \axx{the same as measure. right?}  } $\init:\mathcal{B}(\X)\rightarrow [0,1]$;
    \item $\A$, the set of control inputs with $u\in\A$ as its elements;
    \item $\tr:\X\times\A\times\mathcal B(\X)\rightarrow[0,1]$, a conditional stochastic kernel that assigns to each state $x\in \X$ and control $u\in \A$ a probability measure $\tr(\cdot\mid x,u)$ over $(\X,\mathcal B(\X))$;
  \end{itemize}
  for which $\X$ and $\A$ are (uncountable) Polish spaces.
 \end{definition} 
 
%For any set $A\in \mathcal{B}(\X)$, $\po_{x,u}(x_{t+1}\in A)$ $=\int_A \tr(dy{\,\mid\,} x_{t}=x,u)$, where $\po_{x,u}$ denotes the conditional probability $\po(\cdot\,{\mid}\, x,u)$. At every state the state transition depends non-deterministically on the choice of $u\in \A$.
%When chosen according to a distribution  $\pol_u:\mathcal{B}(\A)\rightarrow [0,1]$, we refer to the stochastic control input as $\pol_u$. 
%The corresponding transition kernel is denoted as $\tr(\cdot| x, \pol_u)=\int_\A \tr(\cdot| x, u) \pol_u(du)\in \mathcal P(\X,\mathcal B(\X))$.
Given a string of inputs
$u(0),$ $u(1), $ $\ldots, $ $u(N)$,
over a finite time horizon $\{0,1,\ldots, N\}$,
and an initial state  $x_0$ (sampled from $\rho$), 
the states $x_{k+1}$ with $k\in \{0,1,\ldots, N\}$
are obtained as a realization of the controlled Borel-measurable stochastic kernel $\tr\left(\cdot{\,\mid\,t} x_k, u_k \right)$ --
these semantics induce paths (or executions) of the MDP. %\axx{relation between N and k?}
  

%\subsubsection{Partially observable Markov decision process (POMDP)}\label{sec:POMDP}
\begin{definition}[Partially Observable  MDP (POMDP)] \label{def:MDP}\mbox{ }\\
A partially observable Markov decision process $\POMDP$ (POMDP) is an MDP $\MDP = (\X, \init, \tr, \A)$  that is partially observable via  
\begin{enumerate}
%	\item $\X$, the state space with states $x\in\X$ as its element;
%	\item $\A$, the action space;
	\item observations $z$ in $\Z$,  which is a Polish space %\axx{what specific feature of Polish speace do you need?} 
	referred to as the observation space, % which is a Borel space.
%\end{enumerate}
%with 
%\begin{enumerate}
%\item $\init$, the initial probability distribution,
%\item $\tr(\cdot|x,a)$, the stochastic transition kernel of the next state given the current state-action pair,
\item $r(\cdot|x)$,  the observation kernel giving the probability of the current observation,  $z\sim r(\cdot|x)$,  for the current state variable $x$.
%\axx{why $r$? in robotic-pomdp literature, this is not common.  $z\sim p_z(\cdot|x)$} => for stochastic kernels I perfer to not use the probability sign
\end{enumerate}


\end{definition} 

An execution of the POMDP  up to time $k$ is given as
\begin{align}\label{eq:history} (x_0,z_0,u_0,x_1,z_1,u_1,\ldots,x_k,z_k,u_k).\end{align}
This sequence is also referred to as the history sequence.
The sequence  \eqref{eq:history} grows with number of observations  $k$ and takes values in the history space $\Hist_k$, which is defined as
  $\Hist_{k+1}=\Hist_{k}\times\X\times\Z\times \A$ with $\Hist_0=\X\times\Z\times \A$.
  %
  
  \axx{why history has x? typically it does not, since x is not observable. Information and "data history" are typically the same. Since you have defined information, why do we even need the H definition?}
  
  
  
  \axx{you defined all spaces wiht "blackboard" style. I suggest you do the same for history and information spaces $\mathbb{H}$ and $\mathbb{I}$}
The control actions $u_k$ can be chosen as a function of the history of inputs and observed outputs.  
Define for $k=0,\ldots,N-1,$ the information spaces as
\[\I_k=\Z_0\times\A_0\times\dots\times\A_{k-1}\times\Z_k.\]
Elements $i_k$ of $\I_k$ are defined as $(z_0,u_0,z_1,u_1,\ldots,u_{k-1},z_k)\in\I_k$ and are referred to as the $k$-th information vector. 

%We denote with $\Pi$ the set of all policies  \citep[Def. 10.4]{bertsekas2004stochastic}  
%\begin{definition}
	An \emph{observation-based} policy for $\POMDP$ is a sequence $\polb=(\pol_0,\ldots,\pol_{N-1})$ such that, for each $k$, $\pol_k(du_k|\init, i_k)$ is a universally measurable stochastic kernel on $\A$  given $\mathcal{P}(\X)\times \I_k$.
	We say that $\polb$ is non-randomized if for all $\init$, $k$, and $i_k$,    $\pol_k(\cdot|\init, i_k)$ is a Dirac distribution. Remark that unless otherwise mentioned, we will assume thay a policy represents a sequence of  stochastic kernels on $\A$. 
 \marginpar{\tiny \axx{why du and not just u?}\red{it is a measure, not a function or a pdf}}

Given the distribution $\init$ for the initial state $x_0$,  and given the theorem of Ionescu Tulcea \citep{hll1996}, there exists a unique probability measure $\P_\pi^\init$ on the canonical space $\Omega:=\Hist_\infty$. \axx{I see where you use history now.} Thus for a given policy the POMDP defines a stochastic process on the probability space  
 $(\Omega,\borel{\Omega},\P_\init^\polb)$.  
 % \begin{align*}
%   P_\pi^\pol(x_0\in B) = \pi(B),\quad P_\pi^\rho(u_n\in C|h_n) = \rho_n(C|h_n),\quad P_\pi^\rho(x_{n+1}\in B|h_n,u_n) = \mathbb T(B|x_n,u_n),
% \end{align*}
%\begin{example}[\red{[to be deleted for submission]}]
%	As a special case of the POMDP, we consider a POMDP represented via difference equations which are subject to process noise and observation noise.
%\begin{align*}
%x_{k+1}&=f(x_k,u_k,w_k)\\
%z_k&=h(x_k,v_k)
%\end{align*}
%where $w_k\sim p_w(\cdot|x_k)$ and $v_k\sim p_v(\cdot|x_k)$ are independent realizations of the the process noise and the observation noise.
%
%
%\end{example}

 \subsubsection{Belief MDP of POMDP}
 Given an information sequence $i_k$ \axx{$I_k$ would be much more common. },  the conditional probability distribution expresses the belief of a state 
\begin{align}
	b_k(\cdot)=\P(x_k\in \cdot|\init,i_k)\in \mathcal P (\X),
\end{align}
thus $	b_k(A) $ defines the likelihood that $x_k$ is in $A$.  %expresses the likelihood of the state
This state distribution is referred to as the belief state $b_k$.  \red{[unresolved comment of Richard. {\it replace . with A ?}]}
The Belief space (i.e., the set of all beliefs) is denoted by $\Xb\subset \mathcal P(\X)$. \axx{why not $\mathbb{B}$ \red{[feel free to change $\backslash Xb$ command]}}
It can be shown that the transitions of this belief state evolve based on a fixed stochastic kernel
\begin{align}\label{eq:trb}
	 b_{k+1}(\cdot)\sim \trb(b_{k+1}\in \cdot|b_k,u_k).
\end{align}
At each time step, the belief state can also be computed using a 
recursive filter denoted by $\tau$ as 
\[b_{k+1}=\tau(b_k,u_k,z_{k+1}).\]

Therefore for a POMDP $\POMDP$, we can construct a belief MDP. 
 That is, a partially observable MDP can be written as a completely observable MDP whose state space is the belief space. We refer to the latter as a belief space model of belief MDP, which is a
discrete-time MDP defined by the tuple $\MB(\POMDP) = (\Xb, \initb, \trb, \A)$ with
  \begin{itemize}
    \item $\Xb$,  a state space with states $b\in\Xb$; % as its elements;
    \item $\initb$, the initial probability distribution $\initb:=\init$;
    \item $\A$, the set of control inputs with $u\in\A$ as its elements;
    \item $\trb:\Xb\times\A\times\mathcal B(\Xb)\rightarrow[0,1]$, a conditional stochastic kernel that assigns to each state $b\in \Xb$ and control $u\in \A$ a probability measure \eqref{eq:trb} over $(\Xb,\mathcal B(\Xb))$;
  \end{itemize}
  for which $\Xb$ and $\A$ are  Polish spaces \citep{bertsekas2004stochastic} and $\trb$ is a Borel measurable mapping.\footnote{In this work, we  require without loss of generality that the belief spaces are Polish. Remark that for finitely parameterizable spaces this always holds. } \marginpar{\tiny \axx{can you assume Polish-ness for teh belief space?} \red{see new footnote}}
  \new{In the sequel, we use $\MB$ and omit the argument $\POMDP$ whenever possible. }%from which the belief MDP $\MB(\POMDP)$ is generated  denote it instead as $\MB$. 

Since the MDP $\MB(\POMDP)$ is a completely observable MDP, its information sequence at time $k$ is given as  
\[i_k=(b_0, u_0, b_1, u_1, \ldots, u_{k-1},b_k).\]
Thus a policy for $\MB(\POMDP)$ %\axx{has $\MB$ defined as a standalone symbol?}
 is a sequence $\polb=(\pol_0,\ldots,\pol_{N-1})$ such that  for all $k$, $\pol_k(du_k|\init, i_k)$ is a universally measurable stochastic kernel on $\A$  \new{parameterized with the initial state distribution and the information sequence, that is  $(\init, i_k)\mathcal{P}(\X)\times \I_k$.} \marginpar{\tiny \axx{I have a little bit of challenge with this product space here.} better?}
	%We say that $\polb$ is non-randomized if for all $\init$, $k$, and $i_k$,    $\pol_k(du_k|\init, i_k)$ is a Dirac distribution.
	We say that $\polb=(\pol_0,\ldots,\pol_{N-1})$ is a Markov policy if for each $k$, the policy only depends on the current state $b_k\in\Xb$, that is $\pol_k(du_k|b_k)$. \marginpar{\tiny\axx{maybe sth like $\pol_k(du_k|b_k)=\pol_k(du_k|b_{0:k})$ is better?} \red{}}
	Furthermore 	 $\polb$ is a stationary Markov policy if $\pol(du_k|b)= \pol_k(du_k|b)$ for all $k$, and $b$.

	


 \section{Linear Temporal Logic for Belief MDPs}
    
  

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Belief space labeling.}\label{sec:DTL}  
%    I kicked out the predicate based logic. Cause it didnt work.
%  Predicate $f\leq 0$ is defined as a function $f:\mathbb{B}\rightarrow \mathbb{R}$ that encodes constraints or properties over belief space. Defining predicates over the belief space allows us to enforce properties directly on the probability distribution of the system (and hence its chance constraints).  
\new{To analyze the behavior of the belief MDP, we introduce some atomic propositions. These propositions are the basic building blocks from which we will build our specifications in the next paragraph.  Consider a set of atomic propositions, denoted $AP$, with elements $p_i$. Each $p_i$ encodes a property on the belief state.  }  
For a belief state characterized by  a Gaussian distribution with the mean $\hat x\in \mathbb R^n $ and variance $P\in\S^n$, examples of such atomic propositions are  
% \new{Parameterize the space of Gaussian distributions with this mean and variance, that is } 
%  $\Xb:=\mathbb R^n\times \S^n$,  then a labeling function  can be for example be composed of level functions based on
 %\axx{$X_b$ was a infinite-dimensional function space. Now, we overloaded it with the finite-dimensional parameter space. I think it is okay. But, we just need to be careful which one we are referring to when we work wiht it.}
     \begin{enumerate}
         \item a position based proposition that is evaluated as true when the  state mean $\hat{x}$ is in a specific (Borel measurable) set  in $\mathbb R^n $. %The set could define a region of interest containing rocks.
   % where in the state space the system should be. 
 	\item an uncertainty based proposition, evaluated as true when the determinant of the state variance is less than a given constant $c_1$, that is   $det(P)\leq c_1$;  %or when the trace of the state variance is bounded 
 %	\item Bounds on determinant or trace of the covariance matrix (i.e., $det(P)$, $Tr(P)$) to  bound the uncertainty about the system's state.
	\item \new{ a proposition $ p_i$ which assigns a lower or upper bound to  the chance of an event $A$,  that is $ p_i \leftrightarrow b_k(A)=\po\left(x_k\in A{\,\mid\,}\rho, i_k\right)>c$. }
% 	\item Bound on projection of covariance matrix $\Pi P$ to bound the uncertainty in a specific direction. 
%    \item Bounds on Mahalanobis distance $\mathcal{M}(\hat{x},P,x) = (\hat{x}-x)^TP^{-1}(\hat{x}-x)$
%    to describe the distance from a point to a Gaussian distribution, when specifying a desired state (or region) in the state space. 
    \end{enumerate}
%     	\axx{when i saw the label, I thought you are going to assign semantic values such as sand, rock, ... But, it is different. So, is "label" the right/common word for this in the LTL world? To me "attribute" or "feature" seems better. }\red{Sofie: labeling is the right word within LTL, we can specify  the type of features that is isolated with the used labeling?} \red{Sofie: I think the above set of examples is easier to understand?}
      \new{At each state, several  atomic propositions can hold. For example, while staying in a region of interest as given in (1), the uncertainty on the current state could also be very low.  }
   \new{We say that a combination of propositions $p_i\in AP$ define an alphabet $\Sigma := 2^{AP}$. Each letter $\letter$ of the alphabet is defined as a set of atomic propositions.  Consider  a labeling function which maps states in the state space to letters in the alphabet. This labeling function is denoted as $\Lab:\Xb\rightarrow \alphabeth$.  }
We require that the labeling function is measurable, that is we require that the induced sets  are Borel measurable
\[B_p:=\{b|\Lab(b)\vDash p \}\in \borel{\Xb}\]    
 Due to the Borel measurability of typical operations in linear algebra,  as been proven in \citep{azoff1974borel},\citep[page 116]{lang1993real}, the examples in this subsection are all Borel measurable.
%For maps on the Euclidean spaces, we also have that the simple algebraic operations  preserve measurability \citep[page 116]{lang1993real}.
 
 \new{ To analyze the dynamic behavior of the system, we capture the evolution of the letters generated by the system (and hence the evolution of the truth value of the atomic propositions) as 
infinite strings. We refer to these strings as words, and denote them as $\word=\letter_0,\letter_1,\letter_2,\ldots\in\alphabeth^{\mathbb{N}}$ with $\mathbb N$ the set of natural numbers.
 %\axx{word repetition} fixed
Of interest to us is the probabilistic evaluation of events over the words generated by traces of the belief models, that is 
$\word:=\Lab(b_0),\Lab(b_1),\Lab(b_2), \ldots$. % and denote the suffix sequence  $\Lab(x_i),$ $\Lab(x_{i+1}), \ldots$ as $\word_i$.
In the next paragraph, we define these events via trace properties given in temporal logic formulae.}
  
  \subsection{Linear temporal logic formulae}
    %Via its trivial extension,  output traces  $\{x_{t}\}_{t\geq 0}\in \X^{\mathbb N} $ can be mapped to the
    

%Define $\BF{b} = b^0b^1 \ldots $%\in \CA{G}^{\omega}$,     and denote the suffix sequence $b^i b^{i+1} \ldots$ by  $\word_i$, $i \geq 0$. 
 % We  combine the predicates via operators to create specifications.
 We define properties as formulas composed of atomic propositions and operators, which includes 
 Boolean ``and" $\andltl$, ``or" $\orltl$, ``not" $\notltl$, and the temporal operators ``until" $\Until$, ``eventually" $\Event$, ``always" $\Always$, and ``next" $\Next$.
    \begin{definition}[LTL Syntax]
    \label{def:gdtl-syntax}
    The {\em syntax} of LTL includes the minimum number of operators to define the logic:
    \begin{equation*}
     \psi :=  \True \ |\ p \ |\ \notltl \psi \ |\ \psi_1 \andltl \psi_2 \ |\ \psi_1 \Until \psi_2 \ |\ \Next \psi
    \end{equation*} 
    where $p\in \AP$.  
    %where the predicates belong to $\Fpred$, that is $f \in \Fpred$. 
    \end{definition}
    For convenience, we often use the additional operators 
    ``or"  $\psi_1 \orltl \psi_2 \equiv  \notltl (\notltl \psi_1 \andltl \notltl \psi_2)$, ``eventually"
    $\Event \psi \equiv \True \Until \psi$, and
   ``always"  $\Always \psi \equiv \notltl \Event \notltl \psi$,
    %\begin{align*}
    %\psi_1 \orltl \psi_2  \Equiv  \notltl (\notltl \psi_1 \andltl \notltl \psi_2) \\
    %\LTLEVENTUALLY \psi  \Equiv \True \LTLUNTIL \psi \\
    %\LTLALWAYS \psi  \Equiv \notltl \LTLEVENTUALLY \notltl \psi
    %\end{align*}
    where $\equiv$ denotes semantic equivalence. 
 The LTL syntax defines the symbols and their correct ordering to form a formulae. %In the following, we define LTL semantics, i.e., the meaning of those symbols.
% \begin{definition}
   % \label{def:gdtl-semantics}
%    Let $\BF{b} = b^0b^1 \ldots \in \mathbb{B}^{\omega}$
%    be an infinite sequence of belief states. $\BF{b} \models \psi$ denotes the event that the word $\BF{b}$ satisfies specification $\psi$.
%    

In contrast the semantics define the interpretation of the formula. For this denote with  $\word_i$ the suffix sequence  $\word_i:= \letter_i, letter_{i+1}, \letter_{i+2}, \ldots$, 
    % Accordingly, 
     then, the {\em semantics} of LTL  formula are defined recursively  over $\word_i$ as
$\word_i \models p   \Equiv p\in\letter_i$, 
    $\word_i \models \notltl \psi  \Equiv\notltl (\word_i \models \psi) $, 
    $\word_i \models \psi_1 \andltl  \psi_2   \Equiv  ( \word_i \models \psi_1 ) \andltl ( \word_i \models \psi_2 ) $, 
    $\word_i \models \psi_1 \orltl  \psi_2   \Equiv  ( \word_i \models \psi_1 ) \orltl ( \word_i \models \psi_2 ) $, 
    $\word_i \models  \psi_1 \Until \psi_2  \Equiv \exists j \geq i \text{ s.t. } (\word_j \models \psi_2 ) $, 
    $  \andltl (\word_k \models \psi_1, \forall k \in \{i, \ldots j-1\})$
    $\word_i \models \Event \psi   \Equiv \exists j \geq i \text{ s.t. }\word_j \models \psi $, and 
    $\word_i \models \Always \psi   \Equiv  \forall j \geq i \text{ s.t. }\word_j \models \psi$.
   % \end{definition}


 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Exact control synthesis for scLTL formulae}
We are interested in the design of a policy for the belief MDP, such that specification given in temporal logics are satisifed with a given probability. 
%\axx{I cannot fully appreciate what is going on in the last two paragraphs here. Needs a story first I guess.} \red{Anticipate this issue, by writing story here.}
\new{
The probability measure $\P_\init^\polb$ over the traces $b_0, b_1, b_2, \ldots\in \Xb^{\mathbb N}$ induces a probability measure over the words $\word:= \Lab(b_0), \Lab(b_1), \Lab(b_2), \ldots\in \Sigma^{\mathbb N}$ generated by the belief MDP.}

Consider the problem to design $\polb$ such that  
\[\P_\init^\polb
(\word \in\Language (\mathcal A_\psi))\geq p\]
where $p$ is either given or to be maximized.

To tackle this problem, we limit ourselves to the syntactically co-safe subset of LTL properties, which can be solved via reachability.  %These properties are of interest as they can be 
%Of interest are properties in the
Consider the syntax of the syntactically co-safe subset of LTL (scLTL) given as
    \begin{equation}\label{eq:scLTL}
     \psi :=  \True \ |\ p \ |\ \notltl p \ |\ \psi_1 \vee\psi_2  \ |\ \psi_1 \andltl \psi_2 \ |\ \psi_1 \Until \psi_2 \ |\ \Next \psi
    \end{equation}     where $p\in \AP$.
\new{ Remark that based on this syntax properties that enforce invariance such as $\Always\psi$ and properties that define liveness specifications, such as $\Always\!\lozenge \psi$, cannot be formed.  } \new{In fact for words that satisfy these properties there always exists a finite time in which the satisfaction of the property is reached, and after which the remaining behavior becomes of no importance for the satisfaction of the property.  This allows us to reformulate the satisfaction as a reachability problem over a DFA. %For this consider the definition of a DFA as given next.
}
% 
%\begin{figure}[htp]
%\centering
%\begin{tikzpicture}[scale=.8]
%	\node (l0) at (-1,0) {};
%	\node[circle, draw, fill=blue!40!gray]  (ll) at (0,0) {\textcolor{white}{$q_1$}};
%		\node[circle, draw, fill=blue!40!gray] (rl) at (1.5,0) {\textcolor{white}{$q_2$}};
%	\node[circle, draw, fill=blue!40!gray] (ru) at (2,1.5) {\textcolor{white}{$q_3$}};
%	\node[circle, draw, fill=blue!40!gray]  (lu) at (0,2) {\textcolor{white}{$q_4$}};
%\node[circle, draw, fill=gray]  (lut) at (3.7,.7) {\textcolor{white}{$q_5$}};
%\path[->,draw]  (l0.center)-> (ll);
%\path[->,draw] (ll) edge[bend right] node[below]  {$p$} (rl)
%(ll) edge[bend left]  node[right]   {$\notltl p$} (lu)
%			   (rl) edge[bend right] node[below right]  {$\notltl p$} (ru)
%			   (lut) edge[bend right] (lu);
%\path[->,draw] (rl) edge[bend right] (ru)
%			   (ru) edge[bend right] node[above] {$p$} (lut)
%	       (ru) edge[bend right] node[above] {$\notltl p$} (ll)
%			   (rl) edge[bend right] node[below]  {$p$} (lut);
%\end{tikzpicture}
%\caption{DFA \red{[write text]}, grey states are  accepting states. Blue states are normal states. }
%\end{figure}

\begin{definition}[Deterministic Finite-State Automaton]
A Deterministic Finite-State Automaton is a tuple
 \[\FSA = (Q, q_0, \Sigma, \delta_\FSA, Acc),\] where %:
$Q$ is a finite set of states, $q_0 \in S$ is the initial state,
$\Sigma$ is the input alphabet with $\sigma\in\Sigma$ being a letter (or event) that triggers the transition between the states of the FSA.   
$\delta_\FSA : Q \times \Sigma \rightarrow Q$ is the transition function, and
$Acc\subset Q$ is a set of accepting states. \hfill \mbox{ }\qed
%\begin{itemize}
%    \item $S_\RA$ is a finite set of states;
%    \item $s_0^\RA \in S_\RA$ is the initial state;
%    \item $\Sigma$ is the input alphabet;
%    \item $\delta : S_\RA \times \Sigma \ra S_\RA$ is the transition function;
%    \item $\Omega_\RA$ is a set of tuples $(\CA{F}_i, \CA{B}_i)$ of disjoint subsets of $S_\RA$ which correspond to good ($\CA{F}_i$) and bad ($\CA{B}_i$) states.
%\end{itemize}
\end{definition}
\new{A word $\word = \letter_0, \letter_1,\ldots,$ is accepted by the DFA if there exists a sequence $q_0,q_1, q_2, .., q_f$ with $q_f\in Acc$, that starts with the start state $q_0$  and for which $q_{i+1}=\delta_{\CA A}(q_i,\letter_i)$.
In other words, a sequence of letters is accepted if the corresponding trace in the DFA {\it reaches} the set of accept states. We denote the set of words accepted by a DFA $\CA A$ as $\Language (\mathcal A)$.}





%\begin{definition}
For every $\psi$ that is expressed with syntactically co-safe LTL  \eqref{eq:scLTL}, there exists a DFA  $\FSA_\psi$ that models the same property \citep{Belta2017}. Specifically, a DFA represents the specification $\psi$ if a word $\word$ satisfies an LTL property $\psi$ if and only if it belongs to the accepting language of the DFA, that is,
	 \[\psi\vDash\word \Leftrightarrow \word\in \Language(\mathcal A_\psi).\]
%\end{definition}

We can
reduce $\P_\init^\polb
(\word \in\Language (\mathcal A_\psi))$  over the traces $\word$ of $\MB$ to a reachability problem
 over another MDP,  denoted  $\MB\otimes\mathcal A_\psi$, which we refer to as a product of the belief MDP $\MB$ and the automaton $\mathcal A_\psi$. This was originally derived in \citep{tmka2013} for MDPs. Since belief MDPs are a instantiation of  MDPs, we give a similar definition of this product construction.
%
%We define the  product  
%as in \citep{tech_report_TACAS}\footnote{\red{This is very similar to the definition in \citep{tmka2013}. }} .
\begin{definition}[Product between automaton and MDP]
\label{def:product}
Given an MDP  $\MDP = (\X, \init, \tr, \A)$,
a finite alphabet $\Sigma$,
a labeling function $\Lab:\X\rightarrow\Sigma$
and a DFA  $\FSA_\psi = (Q, q_0, \Sigma, \delta_{\CA A}, Acc)$,
we define the product between $\MDP$ and $\FSA_\psi$ to be another MDP denoted as
$\MDP\otimes\FSA_\psi = (\bar \X, \bar\init,\bar{\tr},\A)$.
Here $\bar \X= \X\times Q$, $\bar\init(dx,q) = \init(dx)$ if $q= \delta_{\CA A}(q_0,\mathsf L(x))$ else $ \bar\init(dx,q) =0$, and similarly the transition kernel is given as  
\begin{equation*}
  \bar{\tr}(d y\times\{q'\}|x,q,u) =  \left\{\begin{array}{ll}\tr(dy|x,u)& \text{if } q' =\delta(q,\Lab(y)),\\ 0 & \text{else.}  \end{array}\right.
\end{equation*} 
\hfill \mbox{ }\qed
\end{definition}


\new{
We say that a trajectory $
\{\bar x(t)\}_{t\geq 0}$  represents an accepted word for specification $\psi$ if it reaches the set of accept states $K:=\X\times Acc$.
This reachability property can be evaluated over a   trajectory with length $N$ as 
\begin{align*}\textstyle
\exists j \in [0,N]: \bar x(j) \in K.\end{align*}
The probability associated to this finite-time reach  event can be characterized as a boolean expression using indicator functions \cite{Abate1},
which leads to 
%its expression as 
an expectation over the state trajectories as 
\begin{align*} 
r_{\bar x }^\polb(K)=\mathbb{E}_{\bar x(0)}^{\bar \polb}\bigg[\sum\limits_{\mathclap{\qquad j\in[0,N]}}\ind_{K}(\bar x(j))\prod_{i=0}^{j-1}\ind_{\bar{\X}\setminus K}(\bar x(i))\bigg],
\end{align*} 
where $\ind_{B}(x) = 1$ if $x \in B$, 
and otherwise it is equal to $0$.  }

\new{
For a given stationary
%property and
policy $\polb$, the time-dependent value function $\!\mathbf V_\polb^N:\X\rightarrow [0,1]$,  is defined as 
\begin{align}\mathbf V^N_\polb(\bar x)=\mathbb{E}^\polb\!\left[\,\sum\limits_{\mathclap{\ \  j\in [1,N]}} \,\, \ind_K(\bar x(j)) \prod\limits_{\mathclap{i=k+1}}^{j-1}\ind_{\bar{\X}\setminus K}(\bar x(i))\bigg|\bar x(k)=\bar x\right]\!.\label{eq:Valfunc}\end{align}
For a given policy $\mathbf V_\polb^N$ 
expresses the probability that a state trajectory $\{\bar x({0}),\ldots, x({N})\}$,
starting from $\bar x({0})$, will reach the target set $K$ within the time horizon $N$,
while staying within the safe set $A$. 
Hence it expresses the probability of reaching $K$ in $N$ time steps.}
%
\new{This function allows expressing the  reachability probability backward recursively,
as follows. 
Denote the Bellman operator
\begin{align}\label{eq:V_recopt_inf}
& \Bel_\pol (\mathbf  V)(x,q) =\!\!\int_{\bar\X}  \left[\mathbf 1_{K}(\bar x')+  \mathbf 1_{\bar \X\setminus K}( \bar x')\mathbf V( \bar x'))\right]\notag\\&\hspace{5cm}\hfill 
\times \bar{\tr}(d \bar x' |\bar x,\pol(\bar x)),
\end{align}
with $K:=\X \times Acc$.
Then it follows that 
\begin{align}
\mathbf V_\polb^{N} = 
 \Bel_\pol \mathbf V_\polb^{N-1}  .\end{align}
Thus if $\mathbf V_\polb^{N-1} $ expresses the probability of reaching the accept states in $N-1$ then $ \Bel_\pol \mathbf V_\polb^{N} $ expresses the probability of reaching them in $N$.
Every recursion combines the probability to reach the accept states in the next transition with the probability of reaching it in the future.
For the product MDP, this Bellman operator reduces to 
\begin{align}\label{eq:V_recopt_inf_mu}
& \Bel_\pol (\mathbf V)(x,q) =\!\!\int_{\bar\X}  \left[\mathbf 1_{Acc}(q')+  \mathbf 1_{Q\setminus Acc}( q')\mathbf V( x', q'))\right]\notag\\&\hspace{3cm}\hfill 
\times \bar{\tr}(d \bar x' |x,q,\pol(x,q))
\end{align}
with $d \bar x'= d x'\times\{q'\}$. Note that the policy is defined for the product MDP, that is, it defines a control action as a function of the state $(q,x)\in\bar\X$.}
 
%\int_{\bar{\X}}\left[\mathbf 1_{K_\S}(\bar s)+ \mathbf 1_{\S\setminus K_\S}(\bar s) V(\bar s)\right]\bar{\tr}(d\bar s|s,\pol(s)),
%\end{align}
\marginpar{\red{[Richard: Give an explanation of what T represents in words]}}

\new{If we now look at the sequential application of $\mathbf T$, which is denoted by $\mathbf T^x$, we can compute the infinite horizon reachability probability based on the convergence of $\lim_{N\rightarrow \infty}\mathbf T_\mu^{N} \mathbf V$.
Then, for a given stationary policy $\mu$, the  probability of reaching $Acc $, that is $\P_\init^\polb(\MDP\vDash\psi) $, is defined as
\begin{align}
&\P_\init^\polb(\MDP\vDash\psi) = \P_\init^\polb
(\word \in\Language(\mathcal A_\psi))\notag= \\ &\lim_{N\rightarrow \infty}\int_{\bar{\X}}\left[\mathbf 1_{Acc}( q)+ \mathbf 1_{Q\setminus Acc}(q) \Bel^N_\mu\left( \mathbf V\right)(x_0,q)\right] {\bar \init} (d \bar x,q),\label{eq:P:prob}
\end{align}
with $\mathbf V=0$. Remark that a stationary policy $\pol$ for the product MDP can be translated to  time-dependent policy $\polb$ for the original MDP. The latter then implicitly includes the memory function of the product MDP.}

\new{Instead of defining the recursions for a given policy, we can also optimize it with respect to the set of deterministic  policies $\pol: \bar X\times Q\rightarrow \A$ with $\mathbf D_{\pol}$ the set of universally measurable deterministic policies.
This yields the optimal Bellman recursion as
\begin{align}\label{eq:V_recopt_inf}
& \Bel_\ast (\mathbf V)(x,q) =\sup_{\pol\in \mathbf D_{\pol}}\int_{\X}\left[\mathbf 1_{Acc}( q')+  \mathbf 1_{Q\setminus Acc}(q')\mathbf V(x',q'))\right]\notag\\&\hspace{3cm}\hfill\times \bar{\tr}(dx'\times\{q'\}|x,q,\pol(x,q)).
\end{align}
From \cite{Abate1}, we know that the optimal policy of the reachability problem is defined as the argument of the supremum and that the optimal policy is a stationary, universally measurable, and deterministic policy.  }

\new{Hence we have that the maximal satisfaction of a scLTL property $\psi$ reduces to finding the solution of stochastic reachability problem. As a consequence  this reachability problem can be written as follows
\begin{align}
&\sup_{\polb}\P_\init^\polb(\MDP\vDash\psi) = \sup_{\polb }\P_\init^\polb
(\word \in\Language(\mathcal A_\psi))=\label{eq:P:prob_opt} \\ &\lim_{N\rightarrow \infty}\int_{\bar{\X}}\left[\mathbf 1_{Acc}( q)+ \mathbf 1_{Q\setminus Acc}(q) \Bel^N_\ast\left( \mathbf V\right)(x_0,q)\right] {\bar \init} (d \bar x,q).\notag
\end{align}}

\new{Even though we can express the probabilistic satisfaction of these scLTL properties as a stochastic reachability property, we can only perform the computation of the recursions  \eqref{eq:V_recopt_inf_mu} and \eqref{eq:V_recopt_inf}  for a limited set of systems. In general for continuous, or more general uncountable spaces, these recursions are intractable and one needs to use  approximate computations.
This is especially the case for the belief MDPs in this paper.  Hence we can summarize the problem statement of this work next.}

\subsection{Problem statement}
\new{Given a belief MDP $\MB$ and a scLTL specification $\psi$ defined over its belief state, compute a policy $\polb$ which yields a lower bounded satisfaction of $\psi$. That is 
\begin{align}
 \P_\init^\polb(\MB\vDash\psi)\geq p.
\end{align}
In this work, we will tackle this problem by assuming that there exists an approximate model $\tilde \MB$ which is close to the concrete model $\MB$.

}




\section{Refinement-based control synthesis} \label{sec:refinement}
\new{ Let a belief MDP $\MB$ and its abstraction $\tilde \MB$ be given.  
We assume that control synthesis for $\MB$ is intractable, whereas $\tilde \MB$ is computationally amiable for the control synthesis.
In this section, we show  how we can leverage computations performed on the abstract model to verify and control the concrete model. For this we first quantify the accuracy of the abstraction. Then we design a control policy for the abstract model that satisfies the probabilistic scLTL property robustly. Finally, we show that this control policy can be used to construct a control policy for the concrete belief MDP. We will refer to this construction as control refinement. In the next section, we will detail the abstraction for Gaussian  LTI models with partial observations.}



We approach  the control synthesis of the belief MDP in three steps:
\begin{enumerate}
\item We quantify the similarity of the models, for this we introduce the concept of a labeling based $\delta$-stochastic  simulation relation between  the abstract $\tilde{\MB}$ and the concrete $\MB$. Here, $\delta$ quantifies the difference in the stochastic transitions of the two MDPs.
\item Given a specification $\psi$, we are interested in  the $\delta$-robust evaluation of the probability that a specification $\psi$ is satisfied for  $\tilde\MB$.  More precisely, levering the $\delta$-stochastic simulation relations we will use the fact that there exists  a $\delta$-robust evaluation, denoted
$\mathbf{P_{\! rob}}_{\tilde \init}^{\!\ast}(\tilde \MB\vDash\psi; \tilde\Labset,\delta) %\leq  \mathbf P _{\init}^{\polb}(\MDP\vDash\psi).
 %\mathbf R _{\tilde \initb}^{\tilde \polb}(\tilde \MB\vDash\psi),
$ 
such that any control policy ${\tilde \polb}$ for $\tilde \MB$ can be refined to a control policy   ${ \polb}$ for $  \MB$ such that 
 \begin{align}
\mathbf{P_{\! rob}}_{\tilde \init}^{\!\ast}(\tilde \MB\vDash\psi; \tilde\Labset,\delta) 
 %\mathbf R _{\tilde \initb}^{\tilde \polb}(\tilde \MB\vDash\psi)
 \leq  \mathbf P _{\initb}^{\polb}(\MB\vDash\psi).
\end{align}
In the above notation $\tilde\Labset,\delta$ define, respectively, a relaxed labeling for the abstract system and $\delta$ and the deviation in stochastic transitions both introduced in the following subsections.
\item We give a construction of this refinement that hinges on the robust value function computed in (2).
\end{enumerate}




%In this section, we will first introduce the labeling based $\delta$-stochastic simulation relation. Then, in the next subsection we will give a computation which is robust for models in this $\delta$-stochastic simulation relations.  In subsection \ref{sec:control}, we introduce the new concept of value-based control refinement.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lifting based simulation  and approximate similarity}
%\marginpar{\axx{I think in this paper what can help a lot is that:
%
%In each section, start by saying the puprpose of the section, like, In this section, we will ....
%
%And then at the end of the section, in a couple of lines re-emphaize the findings of the section and motivate the next section...
%
%}}
\new{In this subsection, we introduce a similarity relation between MDPs, that is amiable for belief MDPs. 
Let us anticipated this with the introduction of the concept of a relation between two spaces. }
For the sets $A$ and $B$ a relation $\rel\subset A\times B$ is a subset of the Cartesian product $A\times B$. The relation $\rel$ relates $x\in A$ with $y\in B$ if $(x,y)\in\rel$, which is equivalently written as $x\rel y$. 
We use the following notation for the mappings  $\rel(\tilde A):=\{y: x\rel y, x\in \tilde A\}$ and  $\rel^{-1}( \tilde B):=\{x: x\rel y, y\in \tilde B\}$ for   $\tilde A\subseteq A$ and $\tilde B \subseteq B$.

%\begin{definition}[$\delta$-lifting for general state spaces]\label{def:del_lifting}
	Let $\X_1,\X_2$ be two sets with associated measurable spaces $(\X_1,\mathcal B(\X_1)),$ $(\X_2,\mathcal B(\X_2))$,
	and let   $\Delta\in \mathcal{P}(\X_1,\mathcal B(\X_1)) $ and  $\Theta\in \mathcal{P}(\X_2,\mathcal B(\X_2)) $ be two probability measures.  \new{Suppose that $\rel$, defines a set which the values $(x_1,x_2)$ are pairwise `similar' in a certain way, then we can now quantify the similarity of these two measures as in \cite{haesaert2017verification}.}
	%	We denote by\[\bar\rel_\delta\subseteq \mathcal{P}(\X_1,\mathcal B(\X_1))\times \mathcal{P}(\X_2,\mathcal B(\X_2))\]
%	
\begin{definition}[$\delta$-lifting]\label{def:dellifting}
For a given 
	$\rel\subseteq \X_1\times \X_2$ with $\rel\in \mathcal B(\X_1\times \X_2)$, we say that  $\Delta$ and $ \Theta$ are in the corresponding $\delta$-lifted relation, denoted $\Delta \bar \rel_\delta \Theta$  if there exists a probability distribution $\mathbb W$ for the measure space $(\X_1\times \X_2,\mathcal B(\X_1\times \X_2),)$
	satisfying { \setlength{\parskip}{-1pt}\setlength{\parsep}{0pt}
		\begin{description}
			\item[\textbf{L1.}] for all $X_1\in \mathcal{B}(\X_1)$: $\mathbb W(X_1\times \X_2)=\Delta(X_1)$;
			\item [\textbf{L2.}] for all $X_2\in \mathcal{B}(\X_2)$:  $\mathbb W(\X_1\times X_2)=\Theta(X_2)$;
			\item[\textbf{L3.}] for the probability space  $(\X_1\times \X_2,\mathcal B(\X_1\times \X_2), \mathbb W)$ it holds that
			$x_1\rel x_2$ with probability at least $1-\delta$, or equivalently that $\mathbb{W}\left(\rel\right)\geq1-\delta$.
	\end{description}}%
	
We refer to  $\mathbb W$ as the lifting. %We define 
%\end{definition}
The set of related, or equivalently, $\delta$-lifted probabilities is defined as 
	\[\bar\rel_\delta\subseteq \mathcal{P}(\X_1,\mathcal B(\X_1))\times \mathcal{P}(\X_2,\mathcal B(\X_2)). \hfill \mbox{ }\qed\] 

\end{definition}

Hence based on \textbf{L1-3.} in Definition \ref{def:dellifting}, we can quantify the difference between two probability distributions with respect to a relation $\rel$.  \new{Examples of relations are the diagonal $\rel_{\Delta}=\{(x_1,x_2){\mid} x_1=x_2\}$, or a norm based relations $\rel_{\|\cdot\|}=\{(x_1,x_2){\mid} \|Px_1-x_2\|_2\leq \epsilon\}$ with  $P$  a projection matrix and $\epsilon$ a quantification of the precision. }
%\axx{at this point: it is not clear to a reader like me that why this relation has been defined. And how it really quantify the different between to distributions. Maybe one "example relation" can make the definion more intuitive.}
 
 Of interest is the quantification of similarity between two belief MDPs, for this we introduce an approximate probabilistic simulation relation similar to the one in \citep{haesaert2017verification}.
   First we analyze the relation $\rel$ over the (belief) state spaces.
\color{blue} Consider two belief MDPs, the concrete belief MDP $\MB= (\Xb, \initb, \trb, \A)$  and its abstraction $\tilde \MB=(\tilde\Xb, \tilde\initb, \tilde\trb, \tilde\A)$. 
We now want to quantify the similarity between these two stochastic processes.

  For the moment we will assume that a relation $\rel\subset \tilde \Xb \times \Xb$ is given, and we give the requirements for the two systems to be approximately stochastically simulated with respect to this relation. In the next section, we will give an example of such a relation for LTI Gaussian systems. 

As a first requirement, we need that the states in $\rel$ are pair-wise similar, that is, if a certain atomic proposition holds for the concrete model, it should also hold for the abstract model. To this end, we enable set-valued labelings for the abstract model denoted as  $ \tilde{\Labset}(\tilde x):\Xb\rightarrow2^\alphabeth$ and we require that  \begin{equation}\label{req:lab}
  \forall (\tilde x,x )\in \rel:  \Lab(x)\in \tilde{\Labset}(\tilde x)
  \end{equation} with $\Lab:\Xb\rightarrow\alphabeth$ the labeling function of the concrete belief MDP.
Consider 
 the trivial set-valued extension of  the labeling function, that is,  $\Labset:\X\rightarrow2^\alphabeth$ with
 $\Labset(x)=\{\Lab(x)\}$.
Requirement \eqref{req:lab} can now be written as  \begin{equation}
  \forall (\tilde x,x )\in \rel:  \Labset(x)\subset \tilde{\Labset}(\tilde x).
  \end{equation} 
  
Secondly, we require that there exists a $\delta$-lifting for the initial distributions $\initb$ and $\tilde\init$, that is,
\begin{equation}
\tilde\init \bar \rel_\delta \init.
	\tag{\textbf{SR 1.}}
\end{equation}

Finally, we also require that the stochastic transitions of the belief MDPs are approximately similar.  We require that any control action of the abstract MDP can be \emph{refined} to the concrete MDP such that the stochastic transitions are approximately similar. Levering the $\delta$-lifting  we can  express this as follows $\forall (\tilde x, x)\in \rel, \forall \tilde u \in \tilde A, \exists u \in \A$ such that 
\begin{equation}\label{req:translifting}
	\tilde \trb(\cdot| \tilde x, \tilde u)\ \bar \rel_\delta \  \trb(\cdot| x, u).
\end{equation}

To make sure that requirement \eqref{req:translifting} is constructive for the control refinement, we need a more strict condition on the measurability.  Hence, we require that 
  there exists a Borel measurable stochastic kernel $\Wt(\,\cdot\,{\mid} \tilde u,\tilde x,x)$ on $\tilde \Xb\times\Xb$ such that $\forall (\tilde x,x)\in \rel$, $\forall \tilde u\in\tilde \A$:
\begin{equation}\tilde \trb(\cdot| \tilde x, \tilde u)\ \bar \rel_\delta \  \trb(\cdot| x, \InF(\tilde u,\tilde x,x)),\tag{\textbf{SR 2.}}\end{equation} with respect to $\Wt$ and with $\InF$, a given Borel measurable  interface function, mapping the action and state pair to their refined actions
\begin{align*}\InF: \tilde \A\times \tilde \Xb\times\Xb \rightarrow \mathcal{P}(\A,\mathcal B(\A)). \end{align*}

Based on these requirements we are now ready to define a labeling-based $\delta$-stochastic simulation relation as follows. 

\begin{definition}[labeling-based $\delta$-stochastic simulation relation]\label{def:apbsim}
Consider a concrete MDP $\MDP$ and an abstract  MDP $\tilde\MDP$, with labeling maps $\Labset$ and  $\tilde{\Labset}$.   
We say that	$\tilde\MDP$ is $\delta$-stochastically simulated by $\MDP$, that is $$\tilde{\MDP}\preceq^{\delta}_{\tilde{\Labset},\Labset}\MDP,$$ with respect to $(\tilde{\Labset},\Labset)$  if there exists an interface function $\InF$ and
	a Borel measurable relation $\rel\subseteq \tilde \X\times \X$, for which \textbf{SR.1-2} hold and for which 	\begin{equation}
	  \forall (\tilde x,x)\in \rel:  \Labset(x)\subset \tilde{\Labset}(\tilde x)
\tag{\textbf{SR$\,\boldsymbol{\CA{L}}$.}}
	\end{equation} 
holds. \hfill\mbox{ }\qed
\end{definition}

 \color{black}
%  The former notion is defined as follows. 
%Consider a concrete MDP $\MDP$ and an abstract  MDP $\tilde\MDP$, with mappings $h_i$ to a shared {metric} output space  $(\Y,\mathbf{d}_\Y)$. \axx{Define $\Y$, and mention that $\mathbf{d}_\Y$ is the distance metric on $\Y$. When definition $\Y$ explain a bit more the notion of mapping to output space. Are you mapping the latent variable of the MDP? i.e., $y=h(x)\in\Y$. You can state it in equations.} 
%Given a relation over the state spaces of the MDPs we require that the initial distributions $\init$ and $\tilde\init$ can be lifted with $\delta$-accuracy \axx{under which relation? any relation?}
%\begin{equation}
%\tilde\init \bar \rel_\delta \init.
%	\tag{\textbf{SR 1.}}
%\end{equation}
%  
%Furthermore, we require that there exists a Borel measurable stochastic kernel $\Wt(\,\cdot\,{\mid} \tilde u,\tilde x,x)$ on $\tilde \X\times\X$ such that $\forall (\tilde x,x)\in \rel$, $\forall \tilde u\in\tilde \A$:
%\begin{equation}\tilde \tr(\cdot| \tilde x, \tilde u)\ \bar \rel_\delta \  \tr(\cdot| x, \InF(\tilde u,\tilde x,x)),\tag{\textbf{SR 2.}}\end{equation} with $\InF$, a given interface function mapping the action and state pair to a refined action for $\MDP$
%\begin{align*}\InF: \tilde \A\times \tilde \X\times\X \rightarrow \mathcal{P}(\A,\mathcal B(\A)). \end{align*}
%
%Thus the interface function implements (or refines) any control action synthesized over the abstract model to an action for the concrete model.
%\axx{I am lost here :) i do not understand how the interface funciton refines the control action. Even more basic question: what precisely "control action refinement" in this context mean?. Even more-more basic question: why do we need control action refinement? I think the answer to this last question is how we should start writing this part  (i.e., by pointing out an issue and menioning that we need refinement to solve that issue). then propose the interface function.}
%

In \citet{haesaert2017verification}, the difference between the models is quantified based on both a probabilistic error and metric error in an output space of interest. To allow for working with belief space models, we have changed the standard definition such that it now incorporates non-determinism  in the labeling instead of a metric error.
 %In this work we will newly use the computed value function to obtain a refinement.
%\red{[I propose to delete the next definition]}
%\axx{If you end up not deleting it, we should start with some intuition. Why do we need this simulation relation? what does it mean? Is it good for to MDP to be stochastically simulated?}
%\begin{definition}[$\epsilon,\delta$-stochastic simulation relation]\label{def:apbsim}
%Consider a concrete MDP $\MDP$ and an abstract  MDP $\tilde\MDP$, with mappings $h$ and  $\tilde h$  to a shared {metric} output space  $(\Y,\mathbf{d}_\Y)$.   
%	$\tilde\MDP$ is $(\epsilon,\delta)$-stochastically simulated by $\MDP$, denoted by $\tilde\MDP\preceq^{\delta}_\eps\MDP$,  if there exists an interface function $\InF$ and
%	a Borel measurable relation $\rel\subseteq \tilde \X\times \X$, for which \textbf{SR.1-2} hold and for which 
%	\begin{equation}
%		\forall (\tilde x,x)\in \rel:\mathbf{d}_\Y(\tilde h(\tilde x),h(x))  \leq \epsilon.\tag{\textbf{SR\,$\boldsymbol{\epsilon}$}.}
%	\end{equation} 
%\mbox{ }	\hfill\mbox{ }\qed
%\end{definition}
%Condition \textbf{SR$\epsilon$.} is overly conservative for the purpose of this paper. \axx{why?}
%Consider the trivial set-valued extension of  the labeling function $\Lab:\X\rightarrow\alphabeth$, that is  $\Labset:\X\rightarrow2^\alphabeth$ with
% $\Labset(x)=\{\Lab(x)\}$.
%% {\color{blue}A little confusing to use $\Labset$ for both language and labeling map}  => fixed
% 
%We now require that \begin{equation}
%  \forall (\tilde x,x )\in \rel:  \Labset(x)\subset \tilde{\Labset}(\tilde x).
%  \end{equation} 
%  \axx{Is this requirement the result of SR epsilon equation? how? We were talking about relations... what is the connection between "relations" and the "labelling" function? Is labelling function being treated as a mapping to the output space? If yes, I was thinking a bit wrong... when I read output space, I thought we are referring to measurement (e.g., sensory measurement) space. Like the common notion of "output" in basic control theory. Can you just call it "label space" instead of "output space" ?}
%
%\axx{And I still don't get what is the relation yet? it is quite abstract? an example earlier will help so that the reader can project the abstract definition in his/her mind to a concrete example.}
%  Consider the case that $b$ is defined by $\hat x$ and $P$. 
%The choice of $\tilde f =f $  for $f(\cdot):=\det(\cdot)+c_1$, this can be of interest if $\tilde P\succeq P$ implies that
% $f(\tilde b)\leq 0\rightarrow f( b)\leq 0$, then it suffices to show that
% for every state pair  $ (\tilde b,b)\in \rel$ it holds that $\tilde P\succeq P$.
 %Instead, pseudo norms can be leveraged as well as preorders over $b$.  
 
% 
% 
%\begin{definition}[labeling-based $\delta$-stochastic simulation relation]\label{def:apbsim}
%Consider a concrete MDP $\MDP$ and an abstract  MDP $\tilde\MDP$, with labeling maps $\Labset$ and  $\tilde{\Labset}$.   
%We say that	$\tilde\MDP$ is $\delta$-stochastically simulated by $\MDP$, that is $\tilde{\MDP}\preceq^{\delta}_{\tilde{\Labset},\Labset}\MDP$, with respect to $(\tilde{\Labset},\Labset)$  if there exists an interface function $\InF$ and
%	a Borel measurable relation $\rel\subseteq \tilde \X\times \X$, for which \textbf{SR.1-2} hold and for which 	\begin{equation}
%	  \forall (\tilde x,x)\in \rel:  \Labset(x)\subset \tilde{\Labset}(\tilde x)
%\tag{\textbf{SR$\,\boldsymbol{\CA{L}}$.}}
%	\end{equation} 
%holds. \hfill\mbox{ }\qed
%\end{definition}
%

%\axx{summarize the findings of the section, adn say how it connects to the next section...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Robust control computation}

In this section, we define a robust quantification of the probability that a scLTL property is satisfied.   
We introduce robust computations,  based on
 \citet{tech_report_TACAS}, for which the policies are robust to the approximation errors. More precisely,   levering the $\delta$-approximate simulation relation these policies can be refined to the original MDPs', while preserving the satisfaction guarantees.

\new{To robustify the Bellman operator \eqref{eq:V_recopt_inf_mu}, we resolve the non-determinism caused by the set-valued labeling by minimizing over the associated values as}
\begin{align}\label{eq:V_recopt_inf}
& \Bel_\pol^\Labset (\mathbf V)(x,q) \!\!=\!\!\int_{\X} 
\min_{q' \in \delta_\FSA(q, \Labset( x'))}\left[\mathbf 1_{Acc}(q')\right.\notag\\ &\left.\hspace{1.25cm}+  \mathbf 1_{Q\setminus Acc}(q')\mathbf V( x',q'))\right] {\tr}(d x'|x,\pol(x,q)),
\end{align}
and the optimal Bellman 
\begin{align}
& \Bel_\ast^\Labset (\mathbf  V)(x,q) =\sup_\pol\int_{\X}\min_{ q' \in \delta_\FSA(q, \Labset(\bar x))}\left[\mathbf 1_{Acc}( q')\right.\hspace{1.25cm}\notag\\&\left.\hspace{1.5cm}+  \mathbf 1_{Q\setminus Acc}( q')\mathbf V( x', q'))\right]{\tr}(d x'
|x,\pol(x,q)).\label{eq:V_recopt_inf}
\end{align}
\marginpar{\tiny \axx{Here is a good place to contrast the above operators with the ones intriduced in the previous section, to remind the reader about concepts and create a coherent stroy.}}
To create a robust controller, we introduce a robust Bellman operator  $\BelR$. This operator is a  composition of the Bellman operator together with a correction factor $\delta$ and  a truncation operation, that is for $\mathbf V:\X\rightarrow [0,1]$
\begin{align}\label{eq:V_recopt_inf}
& \BelR_\pol^{(\Labset,\delta)} (\mathbf V)(x,q) = \trunc{[0,1]}{\Bel_\pol^\Labset (V)(x,q)-\delta}.
\end{align}
\new{Remark that the truncation clips negative values of the now robustified probabilistic reachability property to zero. }
%\axx{provide more intution about the meaning of R. Why do we have trunctation here?}
Similarly, the robust version of the optimal Bellman operator is given as 
\begin{align}\label{eq:V_recopt_inf}
& \BelR_\ast^{(\Labset,\delta)} (\mathbf V)(x,q)= \trunc{[0,1]}{\Bel_\ast^\Labset (V)(x,q)-\delta}
\end{align}

%The ``robustified'' optimal probability of reaching $Acc_\FSA$ is given as
%\axx{why?}
%\begin{align}
%	V^\infty_\ast & =\lim_{N\rightarrow \infty}\BelR_\ast^N (V) \\
%	\pol^\infty_\ast(x,q)  &= \max_\pol \BelR_\pol (V_\ast^\infty)(x,q) 
%\end{align}
%with $V=0$.
%\axx{what do you mean by $V=0$ ?}

The robustified optimal probability of reaching $Acc$ and hence satisfying $\psi$ is given as
\begin{align}\label{eq:prob:robust_optimal}
&\mathbf{P_{\! rob}}_\init^{\!\ast}(\MDP\vDash\psi; \Labset,\delta) =\\ & \int_{\X}\min_{\bar q \in \delta_\FSA(q_0,\Labset(x_0))}\left[\mathbf 1_{Acc}(\bar q)+ \mathbf 1_{Q \setminus Acc}(\bar q) \mathbf W^\infty_\ast(x_0,\bar q) \right]\notag 
\init (d x_0)-\delta\\
&\mbox{ with }\ \mathbf W^\infty_\ast :=  \lim_{N\rightarrow\infty}[\BelR_\ast^{(\Labset,\delta)}]^N (\mathbf W) \mbox{ and } \mathbf W=0.
\end{align} 
Remark the the robust optimal probability is parameterized with the set-valued labeling and the $\delta$ error in the stochastic transitions.


\begin{prop}\label{prop:1}
Suppose that $\tilde \MDP$  is  $\delta$-stochastically simulated by $\MDP$, that is  $\tilde{\MDP}\preceq^{\delta}_{\tilde{\Labset},\Labset}\MDP,$ with respect to  $({\tilde{\Labset},\Labset})$. 
Then, any control policy ${\tilde \polb}^*$ for $\tilde \MDP$ can be refined to a control policy   ${ \polb}$ for $  \MDP$ such that 
 \begin{align}\label{eq:prob_result}
\mathbf{P_{\! rob}}_{\tilde \init}^{\!\ast}(\tilde \MDP\vDash\psi; \tilde\Labset,\delta)\leq  \mathbf P _{\init}^{\polb}(\MDP\vDash\psi).  \qed
\end{align}
 \end{prop}
The proof of this set of operators and of the given proposition can be derived based on the the proofs in \citep{tech_report_TACAS} constructed for $(\eps,\delta)$- stochastic simulation relations.


\red{[Can be removed:]}
\begin{cor}\label{cor:2} Under the same conditions as in Prop. \ref{prop:1}, for 
any stationary optimal control policy ${\tilde \polb}^*$ for $\tilde \MDP$,  a control policy   ${ \polb}$ for $  \MDP$ such that 
 \begin{align}
\mathbf W^{\tilde \polb*}_{N-1} (\tilde x, q)\leq  \mathbf V^\polb_{N-1} (x,q)
\end{align} can be computed if the states are related, that is,
for $(\tilde x, x ) \in \rel$ and with $\mathbf V^\polb_{N-1} (x)$ computed with equation \eqref{eq:Valfunc} for $\MDP$, and  with $\mathbf W^{\tilde \polb}_{N-1} (x)$ computed with the robust operator $\mathbf R^{(\tilde \Labset, \delta)}_{\tilde\pol^\ast}$.  \qed
 \end{cor}


%
%
%
%\begin{figure}[htp]
%	\centering
%
%\begin{tikzpicture}
%
%\tikzset{model/.style={
%  rectangle,
%  inner sep=0pt,
%  text width=25mm,
%  align=center,
%  draw=black, fill=white,
%  minimum height = 10mm
%  }}
%  
%  \node[model] (filthat) at (4,0.75) {\textbf{apprx.  Filter}}; 
%\node[model] (POMDP) at (0,.75) {\textbf{POMDP}}; 
%
%%\node[label=below:$u$](bl) at (-2,.75) {};
%%\node[label=below:$x$](br) at (2,.75) {};
%\node(be) at (.75,1.25) {};
%\node(bw) at (-.75,1.25) {};
%
%%\path[draw,<-] (be.center) -- node[label=right:$\tilde u$]{} (.75,1.75);
%\path[draw,->] (POMDP) -- node[above] {$y$} (filthat);
%%\path[draw,<-] (bl.center)--(Bhat);
%%\path[draw,<-] (Bhat)--(br);
%\node[model, fill=white](m) at (0,-2) {\textbf{POMDP}};
%\node[model, fill=white](filt) at (4,-2) {\textbf{Filter}};
%
%\node[](ml) at (-.75,-1.5) {};
%\node[label=above:$b$](mr) at (6,-2) {};
%\node[label=above:$\hat b$](filthatr) at (6,.75) {};
%\path[draw,->] (-2.2,.75) -- node[above] {$ u$}(POMDP);
%\path[draw,->] (-1.8,.75) |-  (m);
%\path[draw,->] (filt)--(mr);
%\path[draw,->] (filthat)--(filthatr);
%\path[draw,->] (m)-- node[above] {$y$}(filt);
%\begin{scope}[on background layer]
%\node[model, fit=(ml) (filt) (m),inner sep=2.5mm,label=below:$\mathbf B$, fill=gray!30](B) {};
%\node[model, fit=(filthat) (POMDP),inner sep=2.5mm,label=below:$\hat{\mathbf{B}}$, fill=gray!30](B) {};
%\end{scope}			
%\end{tikzpicture}
%\caption{Control synthesis}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Control refinement}\label{sec:control}
Based on proposition \ref{prop:1}, we already know we can construct a refined policy for the concrete model. In \cite{tech_report_TACAS}, this refined policy is constructed via the composition of the conditional lifting of the $\delta$-stochastic simulation relation and based on the abstract policy $\tilde  \pol^\ast$. 
In this paper, we give an improvement of this construction that is independent of the exact lifting used in the simulation relation. Instead, we use the robust  value function $\mathbf  W^\infty_\ast$ computed for the abstract system.
Given this robust value function  
	and abstract policy  $\tilde  \pol^\ast$. We can compute a refined  concrete policy over the product MDP $\MB\otimes\FSA_\psi$ as
	\begin{align}\label{eq:Valuebased_refined}
		\pol_\ast(x,q):=\, &\InF( \tilde{\pol}(\tilde x, q), \tilde x, x) \\ \mbox{ with } &\tilde x:=\arg\max_{ \tilde x\in
		\rel^{-1}(x)} \mathbf  W^\infty_\ast(\tilde x,q).\notag
	\end{align}
When  $\mathbf  W^\infty_\ast$ has a finite domain, then  the maximization in equation \eqref{eq:Valuebased_refined} is performed over a finite set and does not introduce any measurability issues. 
Remark that as a corollary of Proposition \ref{prop:1}, it also follows that the probability that the scLTL property is satisfied is still lower bounded as in equation \eqref{eq:prob_result}.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian LTI system with partial observations}\label{sec:case}
In this section, we derive an approximate model for a POMDP that can be modeled as a Linear Time Invariant (LTI) model.  We first show that the belief MDP of this LTI model can be given based on the Kalman filter. Then, we introduce a formal abstraction of this model.

Consider an LTI system
 \begin{align} \label{eq:LTI} \begin{aligned}
x_{k+1}&=A x_{k} + B u_k+ w_k\\
z_k&=Cx_k+v_k\end{aligned} \end{align}
with $x\in \mathbb{R}^n$ with stochastic disturbances $w_t\sim \mathcal N(0,\CA W)$, and $v_t\sim \mathcal N(0,\CA V)$. 
Equation  \eqref{eq:LTI}  defines an MDP with state space $\X\subseteq\mathbb R^n$,  initial distribution  $\init:=\mathcal N(x_\init,P_\init)$,  control inputs $u_t\in\mathbb R^m$, and a transition kernel $t$. %defined based on difference eqution  \eqref{eq:LTI}. 
This is a partially observable MDP that can only be observed via  $z_t\in\mathbb R^q$.
 
 
%At $k=0$, we know $x_0\sim \init$ with $\init:=\mathcal N(x_\init,P_\init)$.
Before receiving a measurement $z_0$, the initial state is distributed  as $\CA N(\hat x_{0|-}, P_{0|-})$, with $\hat x_{0|-}:= x_\init$ and $P_{0|-}:= P_{\init}$.
After receiving the measurement $z_0$, this is updated to \begin{align*}
	\hat x_{0|0}&:= x_\init+ L_0 (z_0-Cx_\init),\\
	P_{0|0}&:=(I-L_0 C) P_{\init}(I-L_0 C)^T+L_0\CA V L_0^T,\\
	& \mbox{ with } L_0=P_{\init}C^T\left(CP_{\init}C^T+\mathcal V\right)^{-1},
\end{align*}
with $\po(x_t\in \cdot\,|\,\rho,z_0):=\CA N(\hat x_{0|0}, P_{0|0})$.
This probability distribution defines a belief state as $b_0:=(\hat x_{0|0}, P_{0|0})\in\mathbb R^n\times \mathbb S^n$. The belief space $\Xb$ is  a finite dimensional space and can be parameterized. For example, let $\CA{G}$ denote the Gaussian belief space
    of dimension $n$, i.e. the space of Gaussian
    probability measures over $\BB{R}^n$.
    For brevity, we identify the Gaussian measures
    with their finite parametrization, mean and
    covariance matrix.
     Thus,
    $\Xb \subseteq  \BB{R}^n \times  \Symb^n$.


The dynamics of  $b_k:=(\hat x_{k|k}, P_{k|k})$ are defined via the 
 Kalman filter, that is
	\begin{align*}
	&\text{\it predict: }&\hat x_{k|k-1}&=A\hat x_{k-1|k-1}+Bu_{k-1} \\
	&&P_{k|k-1}&=AP_{k-1|k-1}A^T+\mathcal W,
\\
	%&&\textbf{Update} %\  \qquad e_{k}&=z_k-C \hat x_{k|k-1}\\
	%&&S_k&=CP_{k|k-1}C^T+\mathcal V\\
	%&&
	&\text{\it update: }&\hat x_{k|k}&=\hat x_{k|k-1}+L_k\left(z_k-C \hat x_{k|k-1}\right)\\
	&&P_{k|k}&=(I-L_kC)P_{k|k-1}
	\end{align*}
	with  $L_{k}=P_{k|k-1}C^T\left(CP_{k|k-1}C^T+\mathcal V\right)^{-1}$.
 
This defines a belief MDP $\MB{(\POMDP)}$ with stochastic transitions of the belief state given as 
\begin{align}
	&&\hat x_{k|k}&=A\hat x_{k-1|k-1}+Bu_{k-1}+P_{k|k-1}C^Ts_k\label{eq:beliefx}\\
	&&P_{k|k}&=f(P_{k-1|k-1})\label{eq:beliefP}
\end{align}
with $s_k\sim \mathcal N (0, S_k )$ and  $S_k=\left(CP_{k|k-1}C^T+\mathcal V\right)$. 
\new{The computation of the backwards recursions \eqref{eq:V_recopt_inf} is intractable over the continuous space of this system. Hence the objective is to find a close finite state description of $\MB{(\POMDP)}$.  As a first step, we choose to remove the updates of the covariance matrix $P$.  }
% We define an abstraction of $\MB{(\POMDP)}$  as $\hat\MB$ with state space $\mathbb R^n$ and stochastic transitions
Furthermore in this LTI model,  we replace the stochastic transitions  in equation \eqref{eq:beliefx} by
\begin{align}  
		&&\hat x_k &=A\hat x_{k-1} +B\hat u_{k-1} + \bar P  C^T  \hat{s}_k,\label{eq:abstract} 
\end{align}
with $ \hat{s}_k\sim \CA N (0,\hat{S}_{inv})$ and $\hat{S}_{inv}\preceq S_k^{-1}$ for all $k$ \new{where $\bar P$ defines the steady state $P_{k|k-1}$, i.e., the solution of the Kalman equations. We say that the above system has a state space $\Xb_{x}$.}
These stochastic transitions \eqref{eq:abstract} can then be further abstracted to a finite state model $\hat\MB_{grid}$ by gridding the LTI model.
As in Figure \ref{fig:grid}, model  $\hat\MB_{grid}$ has a finite set of states $ \hat {\X}_{bx}$, denoted $x_s\in \hat {\X}_{bx}\subset  {\X}_{bx}$,  which are representative states that are distributed  equidistantly over the state space $  {\X}_{bx}$.   
Each finite state is the representative point $x_s\in  \hat {\X}_{bx}$ for a cell\footnote{$\oplus$ defines the  Minkowski sum also called the set sum. $\prod$ defines the cartesian product of sets.}
 $\Delta_{x_s}=\{x_s\} \oplus \prod_n [-\grid, \grid]$ such that the whole space is covered by these sets, that is, ${\X}_{bx} \subseteq \bigcup_{x_s\in \hat {\X}_{bx}}\Delta_{x_s}$.


Further $\hat\MB_{grid}$  has transitions 
\begin{align}\label{eq:tgrid}
t_{grid}(s'|s,u)=\hat t \left(\Delta_{s'}\mid x_s, u\right)
\end{align}where $\hat t$ is the stochastic transition kernel associated with difference equation \eqref{eq:abstract}.

\begin{figure}[htp]
\centering
	\includegraphics[width = .6\columnwidth]{figs/grid}
	\caption{Depiction of the gridded state space.The mean state of the concrete belief MDP $\bullet$ and the  representative state of the abstract MDP $\boldsymbol{+}$ are given together with an illustration of a stochastic transition.  \red{Feel free to improve[}}\label{fig:grid}
\end{figure}


 
Consider a simulation relation defined as 
	\begin{align}\label{eq:rel}
\rel := \left\{(x_s,b)| (\hat x-x_s)^T M(\hat x-x_s)\leq \eps^2, \right.\\\qquad\left.  P^-\preceq P \preceq   P^+ \mbox{ with } b=(\hat x, P ) \right\},\notag
	\end{align}
and an interface 
\[\InF(\tilde u,  x_s, \hat x):=K( \hat x - x_s)+\tilde u\]
\marginpar{\axx{interface deserve more explanation and intuition. It is a very key equation/example.}\red{Any suggestion?}}
for some matrices $M, K,P^+,P^-$.  
\new{We now derive the conditions under which $\rel$ and $\InF$ define a labeling-based $\delta$-stochastic simulation relation (c.f. Theorem \ref{def:apbsim}) between $\hat \MB_{grid}$ and $\MB$. That is, we  show that for the relation \eqref{eq:rel}, we can find a set-valued labeling  $\hat \Labset:\hat\X_{bx}\rightarrow 2^\alphabeth$ and that we can compute a $\delta$ such that $\hat \MB_{grid}\preceq^\delta_{\hat\Labset,\Labset}\MB$.}



\subsection{Labeling requirement {\bf SR $\boldsymbol{\Labset}.$}}
We show that  the simulation relation \eqref{eq:rel} enables the use of a set-valued labelling function $\hat \Labset:\hat\X_{bx}\rightarrow 2^\alphabeth$  for the atomic propositions introduced in Section \eqref{sec:DTL}.
To construct this labeling function, we require that for all $x_s$ if there exists
$ b \in \rel(x_s) $ then $ \Lab(b) \in \hat\Labset(x_s)$.

For a position-based proposition $p$ consider, without loss of generality, a labeling $\Lab_p: \Xb \rightarrow \{\{p\},\emptyset\}$ for the concrete belief MDP, denoted
$p\in\Lab_p(b)  \Leftrightarrow \hat x \in A$ with $b=(x,P)$.  The set-valued extension to $\{ \{\{p\},\emptyset\},\{\emptyset\}, \{\{p\}\} \}$
 for the abstract MDP is defined as
 \begin{align}
 	\hat \Labset_p(x_s) =\left\{\begin{array}{ll} \{\{p\}\} & \mbox{ if } \forall \hat x \in \rel (x_s):p\in\Lab_p(b), \\
 	  \{\emptyset \} & \mbox{ if } \forall \hat x \in \rel (x_s):p\not\in\Lab_p(b),\\
 	  \{\{p\},\emptyset\}&\mbox{ else.}\end{array} \right.
 \end{align}
This can be easily computed by shrinking, respectively, expanding the set $A$.  The extension towards more atomic propositions follows naturally.
Similarly, for propositions on the variance of the current belief state any atomic property that is monotonic in $P$ can be mapped to the abstract model.
Finally, for propositions that include probability, one should be more care-full as the quantification of probability of an event is in general not monotonic with the variance.

 
  
  
\subsection{Requirements {\bf SR 1.} and {\bf SR 2.}}
For  {\bf SR 1.}, the initial condition  for the concrete system is given deterministically. as $b_0:=(\hat x_{0|0}, P_{0|0})$. Hence we need to show that there exists an initial state $x_{s,0}$ such that the initial states $(x_{s,0},b_0)\in \rel$.  Thus we require that 
$P^-\preceq  P_{0|0} $ and  $P_{0|0}\preceq P^+$.  Additionally, we choose $x_{s,0}\in \rel^{-1}(\hat x_{0|0})$.    To make sure that the latter is not an empty set, it is sufficient to require that for all $\mathbf r \in \prod_n[-\boldsymbol \delta, \boldsymbol \delta]$ it hols that $ \mathbf r ^T M \mathbf r \leq \eps^2$.
 
 For  {\bf SR 2.},  we need to show that there exists a $\delta$-lifting. 
First we require that $P^+$, and $P^-$ are an upper, respectively, lower bound for $P_{k|k}$ of the belief MDP \eqref{eq:beliefx}-\eqref{eq:beliefP}.  We say that $P^-$ is a lower bound if it is a lower bound for the intial condition (see above) and   if it is monotonically increasing with respect to the Riccati equations see \citep{bitmead1985monotonicity}. For the upper bound, we require, mutatis mutandis, a monotonically decreasing $P^+$. 



We can quantify the difference between $\MB$ and $\hat\MB$ via equation \eqref{eq:rel} by verifying that for all  $(\hat x_k,\hat x_{k|k})\in \rel$ with probability at least $1-\delta$ it holds that $(\hat x_{k+1},\hat x_{k+1|k+1})\in \rel$. 
Consider a choice for the lifted stochastic  transitions  for  dynamics \eqref{eq:abstract} and \eqref{eq:beliefx},  denoted 
	$ \mathbb W_{x}((\hat x_k, \hat x_{k|k})\in \cdot| \hat u_{k-1}, \hat x_{k-1}, \hat x_{k-1|k-1})$, based on the combined stochastic difference equation given as
\begin{align*}
		&&\hat x_{k+1} &=A\hat x_{k} +B\hat u_{k} + \bar P  C^T  \hat{s}_{k+1},\\%\label{eq:abstract3} \\
	&&\hat x_{k+1|k+1}&=A\hat x_{k|k}+Bu_{k}+  \bar P   C^T(  \hat{s}_{k+1}+s^\Delta_{k+1})\notag\\&&&\qquad+\Delta_{k+1}( \hat{s}_{k+1}+ s^\Delta_{k+1})%\label{eq:beliefx3}
\end{align*}
 with $\Delta_k:=(P_{k|k-1}C^T-  \bar P   C^T)$ and with $ \hat{s}_k\sim \CA N (0,\hat{S}_{inv})$ and $ s^\Delta_k\sim  \CA N (0,\  S_k^{-1}-\hat{S}_{inv}). $


We can now choose the lifted stochastic transition kernel 	$\Wt$ for the concrete belief MDP $\MB$ and the abstracted finite MDP $\hat\MB$ as follows.
Denote $b=(\hat x, P)$ and $b_+=(\hat x_+ P_+)$, then $\Wt$ is computed as 
 \begin{align*}
 &	\Wt((x_{s,+},b_+)\in \cdot\,| \tilde u, x_s ,b)\\&:= \left\{\begin{array}{ll} \mathbb W_{x}((\Delta_{x_{s,+}}, \hat x_{+})\in \cdot\,|  \tilde u,x_s , \hat x) &\text{ for }  P_+=f(P)\\
 	0 & \text{ else } \end{array}\right.
 \end{align*}

For this choice of  	$\mathbb W_x$, the difference expression in \eqref{eq:rel} evolves   as 
\begin{align}
 \hat x_{k+1|k+1}-	\hat x_{k+1}=(A+BK)(\hat x_{k|k}-\hat x_{k-1})\qquad \quad\notag\\+  \bar P   C^T s^\Delta_{k+1} +\Delta_{k+1}( \hat{s}_{k+1}+ s^\Delta_{k+1})
\end{align}
 with $\Delta_{k+1}:=(P_{k+1|k}C^T-  \bar P   C^T)$, and with $ \hat{s}_{k+1}\sim \CA N (0,\hat{S}_{inv})$ and $ s^\Delta_{k+1}\sim  \CA N (0,\  S_{k+1}^{-1}-\hat{S}_{inv}). $
For all $ \hat x_{k+1}$, there exists $\mathbf  r \in \prod_n[-\grid ,\grid]$ such that   $\hat x_{k+1}-\mathbf r \in \hat \X_{bx}$. Therefore we can write the update of the difference expression as  \begin{align}
 \hat x_{+|+}-	\hat x_{s_+}=(A+BK)(\hat x_{\,|\,}-\hat x_s)+\mathbf r\qquad \quad\notag\\+  \bar P   C^T s^\Delta_{k+1} +\Delta_{k+1}( \hat{s}_{k+1}+ s^\Delta_{k+1}).
\end{align}
Given that $(\hat x_{\,|\,}-\hat x_s)$ and  $\mathbf r$ belongs to a bounded set, we can bound the influence of the noise terms $s^\Delta_{k+1}$ and $ \hat{s}_{k+1}$ with respect to a probability at least $1-\delta$ for which the update is always in $\rel$ see equation \eqref{eq:rel}.




%    
%Consider the linear time invariant system with Gaussian disturbance, given as 
%\begin{align}\begin{aligned}
%	x^m_{k+1} &= A x^m_{k}+B^mu_{k}+ F^m w_{k}\\
%	y^m_{k}&=C^m x^m_{k}+D^m u_{k}+E^m v_{k}\end{aligned}
%\end{align}
%with matrices $A,B,C,D$ and matrices $F,E$. 
%The measurement noise signal $v$ is zero-mean, independently and identically  distributed noise, i.e, $v_k\sim \mathcal{N}(0,I)$.
%% \noindent\textbf{Wind disturbance.}
%The state transitions are affected by the wind $w_k$. 
%We can model the dynamic variations of the wind as filtered noise. 
%There are two commonly used models  \citep{richardson2013quantifying},  this includes the von Karman power spectral density,
%and the Dryden model.
%The former model matches experiment data more than the 
% Dryden model, but the Dryden model can be represented by  a lower order filter.
%Consider a filter model to be given as
%\begin{align}
%	\begin{aligned}
%	x_{t+1}^w &= A^w x_{t}^w+ F^w e_{t},\\
%	w_{t}&=C^w x_{t}^w+E^w e_{t}.
%	\end{aligned}
%\end{align}
%
%%\textbf{Full model \& Belief space model.}
%The full model is given as follows
%%\begin{align} 
%%	\begin{bmatrix}
%%	x^m_{t+1}	\\x^w_{t+1}
%%	\end{bmatrix}
%% &= \begin{bmatrix}
%% 	A^m 	& F^mC^w\\
%% 	0 & A^w
%% \end{bmatrix}
%%\begin{bmatrix}
%%	x^m_{t}	\\x^w_{t}
%%	\end{bmatrix}+\begin{bmatrix} B^m \\ 0~ \end{bmatrix} u_{t}+  \begin{bmatrix}
%%	F^m E^w \\
%%	F^w
%%	\end{bmatrix}e_{t}\notag\\
%%	y^m_{t}&=\begin{bmatrix} C^m& 0 \end{bmatrix}\begin{bmatrix}
%%	x^m_{t}	\\x^w_{t}
%%	\end{bmatrix}+D^m u_{t}+E^m v_{t}.
%%\end{align}
%%This can be written as 
% \begin{align}  \begin{aligned}
%x_{t+1}&=A x_{t} + B u_t+ Fe_t\\
%y_t&=Cx_t+Du_t+Ev_t\end{aligned} \end{align}
%\begin{align}& \mbox{ with }  x_t	= \begin{bmatrix}
%	x^m_{t}	\\x^w_{t}
%	\end{bmatrix},
%A = \begin{bsmallmatrix}
% 	A^m 	& F^mC^w\\
% 	0 & A^w
% \end{bsmallmatrix}\!, \ 
%B = \begin{bsmallmatrix} B^m \\ 0~ \end{bsmallmatrix}\!,\notag \\
%&F=\begin{bsmallmatrix}
%	F^m E^w \\
%	F^w
%	\end{bsmallmatrix}\!,\ 
%C = \begin{bmatrix} C^m& 0 \end{bmatrix}\!, \ 
%D= D^m, \  E = E^m.\notag
%\end{align}
% 
% \noindent{\textbf{}}
%
%    
  
    
    \section{Case study}
    \input{Casestudy}
  \section{Conclusions}


%% Bibiliography %%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{AliAgha,references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%\appendix
%
%\section{Kalman filtering}
%Consider a Gaussian LTI system:
% \begin{align}  \begin{aligned}
%x_{k+1}&=A x_{k} + B u_t+ w_k,\\
%z_k&=Cx_k+Du_k+v_k.\end{aligned} \end{align}
%with $w_k\sim \mathcal N(0, \mathcal W)$ and $v_k\sim \mathcal N (0,\mathcal V)$.
%
%At $k=0$, we know $x_0\sim \init$ with $\init:=\mathcal N(x_\init,P_\init)$.
%Thus,  before receiving a measurement $z_0$, the distribution of the belief is defined as $\CA N(x_{0|-}, P_{0|-})$
%\begin{align}
%	\hat x_{0|-}&:= x_\init\\
%	P_{0|-}&:= P_{\init}
%\end{align}
%After receiving the measurement $z_0$, this is updated to $\CA N(\hat x_{0|0}, P_{0|0})$
%\begin{align}
%	\hat x_{0|0}&:= x_\init+ L_0 (z_0-Cx_\init)\\
%	P_{0|0}&:=(I-L_0 C) P_{\init}(I-L_0 C)^T+L_0\CA V L_0^T\\
%	& \mbox{ with } L_0=P_{\init}C^T\left(CP_{\init}C^T+\mathcal V\right)^{-1}
%\end{align}
%We represent the belief state  $\CA N(\hat x_{0|0}, P_{0|0})$ by $b_0:=(\hat x_{0|0}, P_{0|0})\in\mathbb R^n\times \mathbb S^n$.
%
%The dynamics of the Kalman filter are given as
%	\begin{align*}
%	&&\textbf{Predict} \qquad \hat x_{k|k-1}&=A\hat x_{k-1|k-1}+Bu_{k-1}\\
%	&&P_{k|k-1}&=AP_{k-1|k-1}A^T+\mathcal W
%\\
%	&&\textbf{Update} \  \qquad e_{k}&=z_k-C \hat x_{k|k-1}\\
%	&&S_k&=CP_{k|k-1}C^T+\mathcal V\\
%	&&L_{k}&=P_{k|k-1}C^TS_k^{-1}\\
%	&&\hat x_{k|k}&=\hat x_{k|k-1}+L_ke_k\\
%	&&P_{k|k}&=(I-L_kC)P_{k|k-1}\\
%	\end{align*}
%	\mbox{Joseph Formula  }
%	\begin{align*}
%	&&P_{k|k}&=(I-L_kC)P_{k|k-1}(I-L_kC_k)^T+L_k\mathcal V_kL_k^T\\
%		\end{align*}
%\mbox{Observability based }
%	\begin{align*}
%	&& P_{k|k}^{-1}&=P_{k|k-1}^{-1}+C_k^T \mathcal  V_k^{-1}C_k
%	\end{align*}
%
%Though the covariance of the belief state is defined as 
%	\begin{align*}
%	&&P_{k|k}&=(I-L_kC)P_{k|k-1}(I-L_kC_k)^T+L_k\mathcal V_kL_k^T, \\
%		\end{align*}
%		The update equations for $P_{k|k-1}$ are more well know:
%			\begin{align*}
%	&&P_{k+1|k}&=(A-K_kC)P_{k|k-1}(A-K_kC_k)^T+K_k\mathcal V_kK_k^T +\CA W
%		\end{align*}
%		with $K_k=AL_k$.
%		
%Hence, the belief state is updated as
%\begin{align}
%	&&\hat x_{k|k}&=A\hat x_{k-1|k-1}+Bu_{k-1}+L_ke_k\\
%	&&P_{k|k}&=f(P_{k-1|k-1})
%\end{align}
%We now want to model the random variable $s_k=L_ke_k$. We know that $s_k$ evolves as a zero mean Gaussian distributed stochastic process.
%Further 
%\begin{align*}
%	\Ex [s_k]=0\\
%	\Ex [s_ks_k^T]=L_k	\Ex [e_ke_k^T]L_k^T, \mbox{ and } \Ex [e_ke_k^T]=S_k \\
%	e_k = C\left(x_k- \hat x_{k|k-1}\right)+v_k\\
%	\Ex [e_ke_k^T] = C P_{k|k-1} C^T + \CA V\\
%		\Ex [s_ks_k^T]=L_k S_k L_k^T,\\
%				\Ex [s_ks_k^T]= P_{k|k-1} C^T S_k^{-1} C P_{k|k-1},\\
%								\Ex [s_ks_k^T]= P_{k|k-1} C^T \left(CP_{k|k-1}C^T+\mathcal V\right)^{-1} C P_{k|k-1},\notag\\
%\Ex [s_ks_k^T]= P_{k|k-1}-P_{k|k}\notag
%\end{align*}
%
%
%\begin{align}
%	&\text{concrete:}\left\{\begin{aligned}
%		&&\hat x_{k|k}&=A\hat x_{k-1|k-1}+Bu_{k-1}+ P_{k|k-1} C^T  \bar{s}_k\notag \\
%		&&&\mbox{with } \bar{s}_k\sim \CA N (0,S_k^{-1}) \notag 
%	\end{aligned}\right.	\\
%&		\text{abstract 1:}\left\{\begin{aligned}
%		&&\hat x_k &=A\hat x_{k-1} +B\hat u_{k-1} + \bar P  C^T  \hat{s}_k\notag \\
%		&&&\mbox{with } \hat{s}_k\sim \CA N (0,\hat{S}_{inv}) \notag 
%	\end{aligned}\right.	
%\end{align}
%Define  $\Delta_k:=P_{k|k-1} -\bar P $,  then  $S_k=\left(C\bar PC^T+C\Delta_kC^T+\mathcal V\right)$.
%Find maximal $\hat{S}_{inv}$ such that $\hat{S}_{inv}\preceq S_k^{-1}$
%


\end{document}


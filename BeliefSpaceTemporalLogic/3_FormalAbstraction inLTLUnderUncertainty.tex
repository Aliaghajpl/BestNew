\documentclass{ifacconf}
\usepackage{standalone}
\usepackage{times}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}      % include this line if your document contains figures
\usepackage{natbib}        % required for bibliography
%===========================================
\input{TemplateFiles/inc}
\input{TemplateFiles/commands.tex} 
\pdfinfo{
   /Author (S.Haesaert et al.)
   /Title  (Formal abstraction of POMDPs for Distribution LTL)
   /CreationDate (D:20101201120000)
   /Subject (Formal abstraction)
   /Keywords (abstraction;POMDP)
}

% Table caption wrangling
\usepackage{etoolbox}
 



\allowdisplaybreaks[1]
%% commenting
\newcommand{\red}[1]{{\color{red} #1}}

\begin{document}


\begin{frontmatter}

\title{\huge Formal abstraction of POMDPs for Distribution LTL}
\thanks[footnoteinfo]{Sponsor and financial support acknowledgment
goes here. Paper titles should be written in uppercase and lowercase
letters, not all uppercase.}

\author[cal]{S. Haesaert} 
\author[cal]{Lars Petter P.  Nilsson} 
\author[mit]{ Cristian-Ioan Vasile}
\author[jpl]{Rohan}
\author[jpl]{ Ali Agh}
\author[cal]{Richard Murray}
\author[cal]{Aaron Ames}

\address[cal]{California Institute of Technology, 
   Pasadena, CA 91125 USA (e-mail: \{haesaert,pettni,ames,murray\}@caltech ).}
\address[mit]{Massachusetts Institute of Technology, 
   Cambridge, MA 02139 USA (e-mail:  cvasile@mit.edu)}
\address[jpl]{Jet Propulsion Laboratory, 
   Pasadena, CA 91109 USA (e-mail: rohan.a.thakker@jpl.nasa.gov)} 
\maketitle
\begin{abstract}
Currently this report mainly includes copies of the  original paper on pulled by Cristi on Temporal logics with beliefs for planning problems.  Goal is to first see how the temporal logic specifications can be defined with measurability in mind. Then we will include this in the original paper, before starting continuing with the development of this report. 
\end{abstract}
\begin{keyword}
Five to ten keywords, preferably chosen from the IFAC keyword list.
\end{keyword}

\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\section{Introduction} \label{subsec:intro}
In this work, we use a language for specifying behavior under uncertainty using formal methods. 
Some of the questions to be tackled are as follows:
\begin{description}
	\item[Framework:] Does it make sense to formulate the DLTL on a specific choice of belief space or sufficient statistic? Can we instead first formulate it on the POMDP and then show that it  is equivalent to a problem on a COMDP?
	\item[Technical:]  What are the minimal measurability conditions that are required such that for given a POMDP and its reduced COMDP  the LTL problem is ``well"-defined (measurable).
	\item[Technical:] Beyond well-definedness, do we also keep soundness and completeness?
	\item[Technical:] When are optimal policies non-randomized?
	\item[Simulation relations]  Use $(\epsilon,\delta)$- simulation relation for doing formal abstractions on the COMDP model.
	Show that it can be used for control refinement.
	\item[Interesting:] How does system identification fit into this picture?
	\item[Simulation relations:] Can we quantify an $(\epsilon,\delta)$-simulation relation between the sampling based model and the original COMDP?
	\item[Computation results:] Improve results for the computation of $(\epsilon,\delta)$-simulation relations for LTI systems. Reformulate LMI requirement from S-procedure to requirements over Polytopes. 
	\item[Scalable computations:] Can we  tackle the computations in a modular fashion. Split up requirements for e.g. the helicopter and the rover and show that the combined system still behaves as required.
	 
\end{description}


\subsection{Literature}
This section contains a summary of related literature.

\subsection{Technical math results for POMDPs}
The reduction of Markov decision problems with incomplete information to problems with complete information has been tackled in
\cite{yushkevich_reduction_1976,rhenius_incomplete_1974}, with respect to Borel states and Borel actions.

In \cite{feinberg2016partially} and in \cite{feinberg2014optimality} the optimality conditions for POMDP problems solved via COMDPs are analyzed. 

In \cite{saldi2017finite}, the convergence of solutions of finite abstractions of COMPDs to the optimal solutions of POMDPs is analyzed. 


Most of the work (including the references given above) has been developed for additive cost functions. 


\subsection{POMDP reachability and safety verification}


In \cite{ding2013optimal}, the optimal control of partially observable systems over safety specifications is analyzed. 
In \cite{LESSER20141989}, the analysis of partially observable systems with as objective reachability is analyzed. 


\subsection{Notation}

For the sets $A$ and $B$ a relation $\rel\subset A\times B$ is a subset of the Cartesian product $A\times B$. The relation $\rel$ relates $x\in A$ with $y\in B$ if $(x,y)\in\rel$, which is equivalently written as $x\rel y$.
%We use the following notation for the mappings $\rel(\tilde A):=\{y: x\rel y,\  x\in \tilde A\}$ and $\rel^{-1}(\tilde B):=\{x: x\rel y,\  y\in \tilde B\}$  for $\tilde A\subseteq A$ and $\tilde B\subseteq B$.




    Let $\Sigma$ be a finite set. The cardinality,
    power set, Kleene- and $\omega$-closures
    of $\Sigma$ are denoted by $|\Sigma|$,
    $2^{\Sigma}$, $\Sigma^*$ and $\Sigma^\omega$,
    respectively.    
    Each member of $\Sigma^*$ and $\Sigma^\omega$ is referred to as ``word" or "sequence". 
    
    
    $A \subseteq \BB{R}^n$ and $B \subseteq \BB{R}^m$,
    $n, m \geq 0$, we denote by $\CA{M}(A, B)$ the set of
    functions with domain $A$ and co-domain $B$, where $A$ has positive measure with
    respect to the Lebesgue measure of $\BB{R}^n$.
    
    
    The set of all positive semi-definite matrices of size
    $n \times n$, $n \geq 1$, is denoted by $\S^n$.    The $m \times n$ zero matrix and
    the $n \times n$ identity matrix are denoted by
    $\BF{0}_{m, n}$ and $\BF{I}_n$, respectively.
    The supremum and Euclidean norms are denoted by
    $\norminf{\cdot}$ and $\normeucl{\cdot}$, respectively.
    For a given set $\Z$ a metric or distance function $\mathbf d_\Z$ is a function $\mathbf{d}_\Z: \Z\times \Z\rightarrow \mathbb R_{\ge 0}$ 
satisfying the following conditions: 
$\forall y_1,y_2,y_3\in\Z$:
$\mathbf d_\Z(y_1,y_2)=0$ iff $y_1=y_2$; 
$\mathbf d_\Z(y_1,y_2)=\mathbf d_{\Z}(y_2,y_1)$;  and
$\mathbf d_\Z(y_1,y_3)\leq \mathbf d_\Z(y_1,y_2) +\mathbf d_\Z(y_2,y_3)$. 
 %
For metric space $\mathbb S$, we denote with  $\mathcal{B}(\mathbb S)$ its Borel $\sigma$-field. That is  $\mathcal{B}(\mathbb S)$ is the  
collection of all sets that can be formed from countable unions and intersections of open sets.
We refer to  $(\mathbb S,\mathcal{B}(\mathbb S))$ as a Borel measurable space and we denote with $\mathcal P(\mathbb S)$ the set of probability measures on $(\mathbb S,\mathcal{B}(\mathbb S))$.
Together with the measurable space $(\mathbb S,\mathcal{B}(\mathbb S))$,  a probability measure $\po$ defines the probability space, denoted by $(\X,\mathcal{B}(\mathbb S),\po)$ and has realizations  $s\sim \po$.   
Polish spaces are complete separable metric spaces, c.f. \cite{bogachev2007measure}. 

    $\BB{E}[\cdot]$ is the expectation operator.

    
 
\section{Partially observable Markov decision Processes}
In this section, we  analyze the formulation of distribution temporal logics as originally given in \cite{JonesDTL2013} for POMDPS. 
First, we give the definition of POMDPs. Then,  we  give the definition of distribution temporal logic. 


The reachability problem can be formulated with a multiplicative cost function or by using a terminal cost function. In the latter case, the terminal function 


\subsection{Markov Decision Processes and Control Policies}
Consider Markov decision processes \citep{Bertsekas2012,mt1993,hll1996}, defined as follows.%
\begin{definition}[general Markov decision process (gMDP)]\label{def:MDP} \mbox{ }\\
  A discrete-time gMDP is a tuple $\M\!=\!(\X,\!\init,\!\tr,\!\U)$ with
  \begin{itemize}
    \item $\X$,  an uncountable Polish state space with states $x\in\X$ as its elements;
    \item $\init$, the initial probability distribution $\init:\mathcal{B}(\X)\rightarrow [0,1]$;
    \item $\U$, the set of control inputs with $u\in\U$ as its elements;
    \item $\tr:\X\times\U\times\mathcal B(\X)\rightarrow[0,1]$, a conditional stochastic kernel that assigns to each state $x\in \X$ and control $u\in \U$ a probability measure $\tr(\cdot\mid x,u)$ over $(\X,\mathcal B(\X))$;
  \end{itemize}
\end{definition}
For any set $A\in \mathcal{B}(\X)$, $\po_{x,u}(x(t+1)\in A)=\int_A \tr(dy\mid x(t)=x,u)$, where $\po_{x,u}$ denotes the conditional probability $\po(\cdot{\mid} x,u)$.
At every state the state transition depends non-deterministically on the choice of $u\in \U$.
When chosen according to
a distribution  $\mu_u:\mathcal{B}(\U)\rightarrow [0,1]$, we refer to the stochastic control input as $\mu_u$. Moreover
the transition kernel is denoted as $\tr(\cdot| x, \mu_u)=\int_\U \tr(\cdot| x, u) \mu_u(du)\in \mathcal P(\X,\mathcal B(\X))$.
Given a string of inputs
$u(0), u(1), \ldots, u(N)$,
over a finite time horizon $\{0,1,\ldots, N\}$,
and an initial condition  $x_0$ (sampled from distribution $\pi$),
the state at the $(t+1)$-st time instant, $x(t+1)$,
is obtained as a realization of the controlled Borel-measurable stochastic kernel $\tr\left(\cdot\mid x(t), u(t) \right)$ --
these semantics induce paths (or executions) of the MDP.
 

 

\subsection{POMDP}\label{sec:POMDP}
\begin{definition}[Partially Observable  MDP (POMDP)] \label{def:MDP}\mbox{ }\\
A discrete-time partially observable Markov decision process (POMDP) with Borel spaces
\begin{enumerate}
	\item $\X$, the state space with states $x\in\X$ as its element;
	\item $\U$, the action space;
	\item $\Z$, observation space.
\end{enumerate}
with 
\begin{enumerate}
\item $\init$, the initial probability distribution,
\item $\tr(\cdot|x,a)$, the stochastic transition kernel of the next state given the current state-action pair,
\item $r(\cdot|x)$,  the observation kernel giving the probability of the current observation,  $z\sim r(\cdot|x)$,  for the current state variable $x$.
\end{enumerate}


\end{definition} 

An execution of the POMDP  up to time $k$ is given as
\begin{align}\label{eq:history} (x_0,z_0,u_0,x_1,z_1,u_1,\ldots,x_k,z_k,u_k).\end{align}
This sequence is referred to as the history sequence.
The sequence  \eqref{eq:history} grows with number of observations  $k$ and will take values in the history space $\Hist_k$, which defined as
$\Hist_0=\X\times\Z\times \U$ and $\Hist_{k+1}=\Hist_{k}\times\X\times\Z\times \U$.


The control can be designed based on the history of inputs and observed outputs.  
Define for $k=0,\ldots,N-1,$ 
\[\I_k=\Z_0\times\U_0\times\dots\times\U_{k-1}\times\Z_k.\]
Elements of $\I_k$ are defined as $(z_0,u_0,z_1,u_1,\ldots,u_{k-1},z_k)\in\I_k$ and are referred to as the $k$-th information vector. 
 

 A policy $\pi$ is a sequence of stochastic kernels on $\U$ given $\I_k$. 
We denote with $\Pi$ the set of all policies.
\citep[Def. 10.4]{bertsekas2004stochastic}  

\begin{definition}
	A policy for $\MDP$ is a sequence $\pi=(\mu_0,\ldots,\mu_{N-1})$ such that, for each $k$, $\mu_k(du_k|\init, i_k)$ is a universally measurable stochastic kernel on $\U$  given $\mathcal{P}(\X)\times \I_k$.
	We say that $\pi$ is non-randomized if for all $\init$, $k$, and $i_k$,    $\mu_k(du_k|\init, i_k)$ is a singleton distribution.
\end{definition}
 Given the distribution $\init$ for the initial state $x_0$,  the theorem of Ionescu Tulcea \citep{hll1996}, there exists a unique probability measure $\P_\pi^\init$ on the canonical space $\Omega:=\Hist_\infty$. Thus for a given policy the POMDP defines a stochastic process on the probability space  
 $(\Omega,\mathcal F,\P_\pi^\mu)$.
% \begin{align*}
%   P_\pi^\mu(x_0\in B) = \pi(B),\quad P_\pi^\rho(u_n\in C|h_n) = \rho_n(C|h_n),\quad P_\pi^\rho(x_{n+1}\in B|h_n,u_n) = \mathbb T(B|x_n,u_n),
% \end{align*}
\begin{example}
	As a special case of the POMDP, we consider a POMDP represented via difference equations which are subject to process noise and observation noise.
\begin{align*}
x_{k+1}&=f(x_k,u_k,w_k)\\
z_k&=h(x_k,v_k)
\end{align*}
where $w_k\sim p_w(\cdot|x_k)$ and $v_k\sim p_v(\cdot|x_k)$ are independent realizations of the the process noise and the observation noise.


\end{example}

 \subsection{Belief space}
Due to the noisy and partial observation of the state, the best one can infer about the system state at time  $t$ is a probability distribution over its possible states
\begin{align}
	b_k(\cdot)=\P(x_k\in \cdot|\I_k)\in \mathcal P (\X)
\end{align}
This state distribution is referred to as the belief state $b_t(\cdot)$. 
The Belief space (i.e., the set of all beliefs) is denoted by $\mathbb{B}\subset \mathcal P(\X)$.
It can be shown that the transitions of this belief state evolve based on a fixed stochastic kernel
\begin{align}
	 b_{k+1}(\cdot)\sim \trb(b_{k+1}\in \cdot|b_k,u_k).
\end{align}
At each time step the belief state can also be computed using a 
recursive filter denoted by $\tau$ as 
\[b_{k+1}=\tau(b_k,u_k,z_{k+1}).\]



A control policy developed for the Belief space model is a mapping from belief space to the action space, i.e., $u_k=\pi(b_k)$. 

\red{[Write about the Borel space etcetera based on \cite{bertsekas2004stochastic}.]}



\red{[MDP description of COMDP]}

 

\section{Belief Space Temporal Logic}

	In this section, we define a language for specifying behavior under uncertainty using formal methods. 
	
	
	
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
    
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Predicates}\label{sec:DTL}  
    
  
    
     Predicate $f\leq 0$ is defined as a function $f:\mathbb{B}\rightarrow \mathbb{R}$ that encodes constraints or properties over belief space. Defining predicates over the belief space allows us to enforce properties directly on the probability distribution of the system (and hence its chance constraints). 
     We require that the predicate functions $f$ are Borel measurable, that is we require that the induced sets  are Borel measurable
     \[B_f:=\{b|f(b)\leq 0\}\in \borel{\mathbb B}\] 
     

     
     Consider a belief state defined by the mean $\hat x\in \mathbb R^n $ and variance $P\in \S^n$ on state $x$. More specifically $\mathbb B:=\mathbb R^n\times \S^n$,  then examples of predicate functions are  \begin{enumerate}
 	\item Bounds on determinant or trace of of the covariance matrix (i.e., $det(P)$, $Tr(P)$) to  bound the uncertainty about the system's state.
 	\item Bound on projection of covariance matrix $\Pi P$ to bound the uncertainty in a specific direction.
   \item   Bounds on state mean $\hat{x}$ to specify
    where in the state space the system should be. 
    \item Bounds on Mahalanobis distance $\mathcal{M}(\hat{x},P,x) = (\hat{x}-x)^TP^{-1}(\hat{x}-x)$.
    to describe the distance from a point to a Gaussian distribution, when specifying a desired state (or region) in the state space. 
    \end{enumerate}
 
    The Borel measurability of typical operations in linear algebra 
   has been proven in \cite{azoff1974borel}.
For maps on the Euclidean spaces, we also have that the simple algebraic operations are preserve measurability \cite[page 116]{lang1993real}.

\red{[Require $f$ to belong to a finite set $\Fpred$. ]}
  
    \subsection{DTL}    
    If $\BF{b} = b^0b^1 \ldots \in \CA{G}^{\omega}$,
    we denote the suffix sequence $b^i b^{i+1} \ldots$ by
    $\BF{b}^i$, $i \geq 0$.  \red{[Even when finitely parameterized this is not a finite set, hence $\CA{G}^{\omega}$ operator is not defined. ]}
   We  combine the predicates via operators to create specifications. Operators include boolean "and" $\andltl$, "or" $\orltl$, "not" $\notltl$, and temporal operators: "until" $\Until$, "eventually" $\Event$, "always" $\Always$, "next" $\Next$.
      For the language grammar we will rely on Bakus-Naur form. $\True$ and $\False$ are Boolean constants that respectively describe specifications that are always satisfied or can never be satisfied.
    
    \begin{definition}[\DTL Syntax]
    \label{def:gdtl-syntax}
    The {\em syntax} of \DTL includes the minimum number of operators to define the logic:
    \begin{equation*}
     \phi :=  \True \ |\ f \leq 0 \ |\ \notltl \phi \ |\ \phi_1 \andltl \phi_2 \ |\ \phi_1 \Until \phi_2 \ |\ \Next \phi
    \end{equation*} where the predicates belong to $\Fpred$, that is $f \in \Fpred$. 
    \end{definition}

    For convenience, we define the additional operators:
    $\phi_1 \orltl \phi_2 \equiv  \notltl (\notltl \phi_1 \andltl \notltl \phi_2)$,
    $\Event \phi \equiv \True \Until \phi$, and
    $\Always \phi \equiv \notltl \Event \notltl \phi$,
    %\begin{align*}
    %\phi_1 \orltl \phi_2 & \equiv  \notltl (\notltl \phi_1 \andltl \notltl \phi_2) \\
    %\LTLEVENTUALLY \phi & \equiv \True \LTLUNTIL \phi \\
    %\LTLALWAYS \phi & \equiv \notltl \LTLEVENTUALLY \notltl \phi
    %\end{align*}
    where $\equiv$ denotes semantic equivalence. \DTL syntax defines the symbols and their correct ordering to form a formulae. In the following, we define \DTL semantics, i.e., the meaning of those symbols.

    \begin{definition}[\DTL Semantics]
    \label{def:gdtl-semantics}
    Let $\BF{b} = b^0b^1 \ldots \in \mathbb{B}^{\omega}$
    be an infinite sequence of belief states. $\BF{b} \models \phi$ denotes the event that the word $\BF{b}$ satisfies specification $\phi$.
    
     Accordingly, the {\em semantics} of \DTL is defined recursively as
    \begin{align*}
    &\BF{b}^i \models  \top  & \\
    &\BF{b}^i \models f \leq 0 & \Equiv\quad & f(b^i) \leq 0\\ % \forall (x,P) \in b^i \\
    &\BF{b}^i \models \notltl \phi & \Equiv\quad & \notltl (\BF{b}^i \models \phi) \\
    &\BF{b}^i \models \phi_1 \andltl  \phi_2  & \Equiv\quad & ( \BF{b}^i \models \phi_1 ) \andltl ( \BF{b}^i \models \phi_2 ) \\
    &\BF{b}^i \models \phi_1 \orltl  \phi_2  & \Equiv\quad & ( \BF{b}^i \models \phi_1 ) \orltl ( \BF{b}^i \models \phi_2 ) \\
    &\BF{b}^i \models  \phi_1 \Until \phi_2 & \Equiv\quad & \exists j \geq i \text{ s.t. } ( \BF{b}^j \models \phi_2 ) \\
    & & & \andltl (\BF{b}^k \models \phi_1, \forall k \in \{i, \ldots j-1\})\\
    &\BF{b}^i \models \Event \phi  & \Equiv\quad & \exists j \geq i \text{ s.t. } \BF{b}^j \models \phi \\
    &\BF{b}^i \models \Always \phi  & \Equiv\quad & \forall j \geq i \text{ s.t. } \BF{b}^j \models \phi
    \end{align*}
    
    \end{definition}



\section{Approximate control synthesis}
\subsection{Synthesis problem}
Let a belief space model $\MB$ and its abstraction $\tilde \MB$ be given.  Given a specification $\psi$ defined on $\MB$, how now question
\begin{itemize}
	\item How can we compute a controller for $\MB$ based on $\tilde \MB$,
	\item How can we refine an abstract controller designed for $\tilde\MB$ to $\MB$.
\end{itemize}


We introduce a notion  of approximate probabilistic simulation relations which naturally leads to control refinement.



\subsection{Lifting based simulation}

%\begin{definition}[$\delta$-lifting for general state spaces]\label{def:del_lifting}
	Let $\X_1,\X_2$ be two sets with associated measurable spaces $(\X_1,\mathcal B(\X_1)),$ $(\X_2,\mathcal B(\X_2))$,
	and let   $\Delta\in \mathcal{P}(\X_1,\mathcal B(\X_1)) $ and  $\Theta\in \mathcal{P}(\X_1,\mathcal B(\X_2)) $ be two probability distributions. 
	%	We denote by\[\bar\rel_\delta\subseteq \mathcal{P}(\X_1,\mathcal B(\X_1))\times \mathcal{P}(\X_2,\mathcal B(\X_2))\]
	
For a given 
	$\rel\subseteq \X_1\times \X_2$ with $\rel\in \mathcal B(\X_1\times \X_2)$, we say that  $\Delta$ and $ \Theta$ are in the corresponding $\delta$-lifted relation, denoted $\Delta \bar \rel_\delta \Theta$  if there exists a probability distribution $\mathbb W$ for the measure space $(\X_1\times \X_2,\mathcal B(\X_1\times \X_2),)$
	satisfying { \setlength{\parskip}{-1pt}\setlength{\parsep}{0pt}
		\begin{description}
			\item[\textbf{L1.}] for all $X_1\in \mathcal{B}(\X_1)$: $\mathbb W(X_1\times \X_2)=\Delta(X_1)$;
			\item [\textbf{L2.}] for all $X_2\in \mathcal{B}(\X_2)$:  $\mathbb W(\X_1\times X_2)=\Theta(X_2)$;
			\item[\textbf{L3.}] for the probability space  $(\X_1\times \X_2,\mathcal B(\X_1\times \X_2), \mathbb W)$ it holds that
			$x_1\rel x_2$ with probability at least $1-\delta$, or equivalently that $\mathbb{W}\left(\rel\right)\geq1-\delta$.
	\end{description}}%
	
We refer to  $\mathbb W$ as the lifting. We define 
%\end{definition}
the set of related, or equivalently lifted, probabilities as 
	\[\bar\rel_\delta\subseteq \mathcal{P}(\X_1,\mathcal B(\X_1))\times \mathcal{P}(\X_2,\mathcal B(\X_2))\] 


Hence based on \textbf{L1-3.}, we can quantify the difference between two probability distributions with respect to a relation $\rel$.

\subsection{Synthesis problem}
Let a belief space model $\MB$ and its abstraction $\tilde \MB$ be given.  Given a specification $\psi$ defined on $\MB$, how now question
\begin{itemize}
	\item How can we compute a controller for $\MB$ based on $\tilde \MB$,
	\item How can we refine an abstract controller designed for $\tilde\MB$ to $\MB$.
\end{itemize}


We introduce a notion  of approximate probabilistic simulation relations which naturally leads to control refinement.


\begin{figure}[htp]
	\centering

\begin{tikzpicture}

\tikzset{model/.style={
  rectangle,
  inner sep=0pt,
  text width=25mm,
  align=center,
  draw=black, fill=white,
  minimum height = 10mm
  }}
  
  \node[model] (C) at (0,2.25) {$\tilde{ \mathbf{ C}}$}; 
\node[model] (Bhat) at (0,.75) {$\tilde{ \mathbf{ B}}$}; 

%\node[label=below:$u$](bl) at (-2,.75) {};
%\node[label=below:$x$](br) at (2,.75) {};
\node(be) at (.75,1.25) {};
\node(bw) at (-.75,1.25) {};

\path[draw,<-] (be.center) -- node[label=right:$\tilde u$]{} (.75,1.75);
\path[draw,->] (bw.center) -- node[left] {$\tilde b$} (-.75,1.75);
%\path[draw,<-] (bl.center)--(Bhat);
%\path[draw,<-] (Bhat)--(br);
\node[model, fill=white](m) at (0,-2) {$\M$};
\node[](ml) at (-.75,-1.5) {};
\node[label=below:$y$](mr) at (2,-2) {};
\path[draw,->] (-.75,-1.125) -- node[left] {$\tilde b$}(-.75,0);
\path[draw,<-] (.75,-1.5) -- node[left] {$\tilde u$}(.75,.25);
\path[draw,->] (m)--(mr);
\begin{scope}[on background layer]
\node[model, fit=(ml) (mr) (m),inner sep=2.5mm,label=below:$\mathbf B$, fill=gray!30](B) {};
\node[model, fit=(C) (Bhat),inner sep=2.5mm, fill=gray!30](B) {};
\end{scope}			
\end{tikzpicture}

\caption{Control synthesis}
\end{figure}


 For this, we build on the notion of \emph{interface function} \cite{Girard2009} to define probabilistic simulation relations that allow for
the hierarchical control refinement of two gMDPs
\begin{align*}\InF: \U_1\times \X_1\times\X_2 \rightarrow \mathcal{P}(\U_2,\mathcal B(\U_2)).\end{align*}
This interface function  $\InF$  is required  %in addition
to be a Borel measurable function.
%Thus given $(u_1,x_1,x_2)\in\A_1\times \X_1\times\X_2$, $\InF$ induces a Borel measurable stochastic kernel over $\A_2$.
Intuitively, an interface function implements (or refines) any control action synthesized over the abstract model to an action for the concrete model.


%
\begin{definition}[$\epsilon,\delta$-stochastic simulation relation]\label{def:apbsim}
	Consider two gMDPs $\M_i=(\X_i,\init_i ,\mathbb T_i,\U_i),$ $i =1,2$,  with mappings $h_i$ to a shared {metric} output space  $(\Y,\mathbf{d}_\Y)$.
	$\M_1$ is $(\epsilon,\delta)$-stochastically simulated by $\M_2$ if there exists an interface function $\InF$ and
	a relation $\rel\subseteq \X_1\times \X_2$, for which there exists a Borel measurable stochastic kernel $\Wt(\,\cdot\,{\mid} u_1,x_1,x_2)$ on $\X_1\times\X_2$ given $\U_1\times\X_1\times\X_2$,
	such that. \textbf{SR.1-3} hold.
	{ \setlength{\parskip}{-2pt}\setlength{\parsep}{-1pt}
		\begin{description}
			\item[\textbf{SR2.}] $\forall (x_1,x_2)\in \rel$, $\forall u_1\in\U_1$:
			\[\mathbb T_1(\cdot| x_1, u_1)\ \bar \rel_\delta \  \mathbb T_2(\cdot| x_2, \InF(u_1,x_1,x_2)),\] with lifted probability measure $\Wt(\,\cdot\,{\mid}u_1,x_1,x_2)$;
						\item[\textbf{SR3.}] $\forall (x_1,x_2)\in \rel$,  $f(b_1)\leq0\rightarrow f(b_2)\leq 0$.
	\end{description} }
	\noindent \red{The simulation relation is denoted as $\M_1\preceq^{\delta}_\eps\M_2$.}
\end{definition}
This definition extends the known exact notions of probabilistic simulation in \cite{larsen1991bisimulation},
and the approximate notions of \cite{Desharnais2008,cDAK12} to gMDPs over Polish spaces as elaborated in \cite{haesaert2017verification}.
%
 Introduce a concept of similarity which relaxes the requirement of equality of the output space. 
 Instead we can  require that for 
 \[\forall (\tilde b,b)\in \rel: f(\tilde b)\leq 0\rightarrow f( b)\leq 0.\]
In the above  $\tilde b$ is the approximate belief state. 
As an alternative we can require that there exist  $\tilde f$ such that
 \[\forall (\tilde b,b)\in \rel: \tilde f(\tilde b)\leq 0\rightarrow f( b)\leq 0.\]
  Consider the case that $b$ is defined by $\hat x$ and $P$. Suppose that  
 $f(\tilde b)\leq 0\rightarrow f( b)\leq 0$ if  $\tilde P\succeq P$, then it suffices to show that
 for every state pair  $ (\tilde b,b)\in \rel$ it holds that $\tilde P\succeq P$. This is for instance the case for $f(\cdot):=\det(\cdot).$
 
Hence by replacing the strict requirement on (approximate) equality, with that of order,   we are not restricted to the use of a distance measure. %Instead, pseudo norms can be leveraged as well as preorders over $b$. 
 


\section{Distribution LTL control of POMDPs. }


\begin{figure}[htp]
	\centering

\begin{tikzpicture}

\tikzset{model/.style={
  rectangle,
  inner sep=0pt,
  text width=25mm,
  align=center,
  draw=black, fill=white,
  minimum height = 10mm
  }}
  
  \node[model] (C) at (0,2.25) {$\tilde{ \mathbf{ C}}$}; 
\node[model] (Bhat) at (0,.75) {$\tilde{ \mathbf{ B}}$}; 

%\node[label=below:$u$](bl) at (-2,.75) {};
%\node[label=below:$x$](br) at (2,.75) {};
\node(be) at (.75,1.25) {};
\node(bw) at (-.75,1.25) {};

\path[draw,<-] (be.center) -- node[label=right:$\tilde u$]{} (.75,1.75);
\path[draw,->] (bw.center) -- node[left] {$\tilde b$} (-.75,1.75);
%\path[draw,<-] (bl.center)--(Bhat);
%\path[draw,<-] (Bhat)--(br);
\node[model, fill=white](m) at (0,-2) {$\M$};
\node[](ml) at (-.75,-1.5) {};
\node[label=below:$y$](mr) at (2,-2) {};
\path[draw,->] (-.75,-1.125) -- node[left] {$\tilde b$}(-.75,0);
\path[draw,<-] (.75,-1.5) -- node[left] {$\tilde u$}(.75,.25);
\path[draw,->] (m)--(mr);
\begin{scope}[on background layer]
\node[model, fit=(ml) (mr) (m),inner sep=2.5mm,label=below:$\mathbf B$, fill=gray!30](B) {};
\node[model, fit=(C) (Bhat),inner sep=2.5mm, fill=gray!30](B) {};
\end{scope}			
\end{tikzpicture}

\caption{Control synthesis}
\end{figure}




\begin{figure}[htp]
	\centering

\begin{tikzpicture}

\tikzset{model/.style={
  rectangle,
  inner sep=0pt,
  text width=25mm,
  align=center,
  draw=black, fill=white,
  minimum height = 10mm
  }}
  
  \node[model] (filthat) at (4,0.75) {\textbf{apprx.  Filter}}; 
\node[model] (POMDP) at (0,.75) {\textbf{POMDP}}; 

%\node[label=below:$u$](bl) at (-2,.75) {};
%\node[label=below:$x$](br) at (2,.75) {};
\node(be) at (.75,1.25) {};
\node(bw) at (-.75,1.25) {};

%\path[draw,<-] (be.center) -- node[label=right:$\tilde u$]{} (.75,1.75);
\path[draw,->] (POMDP) -- node[above] {$y$} (filthat);
%\path[draw,<-] (bl.center)--(Bhat);
%\path[draw,<-] (Bhat)--(br);
\node[model, fill=white](m) at (0,-2) {\textbf{POMDP}};
\node[model, fill=white](filt) at (4,-2) {\textbf{Filter}};

\node[](ml) at (-.75,-1.5) {};
\node[label=above:$b$](mr) at (6,-2) {};
\node[label=above:$\hat b$](filthatr) at (6,.75) {};
\path[draw,->] (-2.2,.75) -- node[above] {$ u$}(POMDP);
\path[draw,->] (-1.8,.75) |-  (m);
\path[draw,->] (filt)--(mr);
\path[draw,->] (filthat)--(filthatr);
\path[draw,->] (m)-- node[above] {$y$}(filt);
\begin{scope}[on background layer]
\node[model, fit=(ml) (filt) (m),inner sep=2.5mm,label=below:$\mathbf B$, fill=gray!30](B) {};
\node[model, fit=(filthat) (POMDP),inner sep=2.5mm,label=below:$\hat{\mathbf{B}}$, fill=gray!30](B) {};
\end{scope}			
\end{tikzpicture}

\caption{Control synthesis}
\end{figure}





\section{Case studies}
 We detail two cases. In the first, we give a standard Gaussian LTL model and we show that one can use a constant estimator even when the noise is not constant.
 
\subsection{Flying in the wind: LTI Gaussian case}



\noindent\textbf{Point mass moving.}
    
Consider the linear time invariant system with Gaussian disturbance, given as 
\begin{align}\begin{aligned}
	x^m_{t+1} &= A x^m_{t}+B^mu_{t}+ F^m w_{t}\\
	y^m_{t}&=C^m x^m_{t}+D^m u_{t}+E^m v_{t}\end{aligned}
\end{align}
with matrices $A,B,C,D$ and matrices $F,E$. 
The disturbance signal $v$ is zero mean independently and identically  distributed noise, i.e, $w_k\sim \mathcal{N}(0,I)$.

 
 \noindent\textbf{Wind disturbance.}
We can model the dynamic variations of the wind as filtered noise. 
There are two commonly used models  \citep{richardson2013quantifying},  this includes
\begin{itemize}
	\item the von Karman power spectral density,
	\item the Dryden model.
\end{itemize}
 
 Though the former model matches experiment data more than the 
 Dryden model, the Dryden model can be represented by  a lower order filter.
 
Consider a filter model to be given as
\begin{align}
	\begin{aligned}
	x_{t+1}^w &= A^w x_{t}^w+ F^w e_{t},\\
	w_{t}&=C^w x_{t}^w+E^w e_{t}.
	\end{aligned}
\end{align}

\textbf{Full model \& Belief space model.}
The full model is given as follows
\begin{align} 
	\begin{bmatrix}
	x^m_{t+1}	\\x^w_{t+1}
	\end{bmatrix}
 &= \begin{bmatrix}
 	A^m 	& F^mC^w\\
 	0 & A^w
 \end{bmatrix}
\begin{bmatrix}
	x^m_{t}	\\x^w_{t}
	\end{bmatrix}+\begin{bmatrix} B^m \\ 0~ \end{bmatrix} u_{t}+  \begin{bmatrix}
	F^m E^w \\
	F^w
	\end{bmatrix}e_{t}\notag\\
	y^m_{t}&=\begin{bmatrix} C^m& 0 \end{bmatrix}\begin{bmatrix}
	x^m_{t}	\\x^w_{t}
	\end{bmatrix}+D^m u_{t}+E^m v_{t}.
\end{align}
This can be written as 
 \begin{align}  \begin{aligned}
x_{t+1}&=A x_{t} + B u_t+ Fe_t\\
y_t&=Cx_t+Du_t+Ev_t\end{aligned} \end{align}
\begin{align}& \mbox{ with }  x_t	= \begin{bmatrix}
	x^m_{t}	\\x^w_{t}
	\end{bmatrix},
A = \begin{bmatrix}
 	A^m 	& F^mC^w\\
 	0 & A^w
 \end{bmatrix}, \ 
B = \begin{bmatrix} B^m \\ 0~ \end{bmatrix}\notag \\
&F=\begin{bmatrix}
	F^m E^w \\
	F^w
	\end{bmatrix},\ 
C = \begin{bmatrix} C^m& 0 \end{bmatrix} , \ 
D= D^m, \  E = E^m.\notag
\end{align}
 
 
The belief space $\mathbb{B}$ is  a finite dimensional space and can be parameterized . For example, let $\CA{G}$ denote the Gaussian belief space
    of dimension $n$, i.e. the space of Gaussian
    probability measures over $\BB{R}^n$.
    For brevity, we identify the Gaussian measures
    with their finite parametrization, mean and
    covariance matrix.
     Thus,
    $\CA{G} =  \BB{R}^n \times  \S^n$.
    
    
 
 In the second case, we leverage the developed simulation relation for FIRM. 
 
 \red{[SEARCHING FOR A GOOD CASE! What about the rover? Or rover driving over uncertain map elements?]}



%% Bibiliography %%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{AliAgha,references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}


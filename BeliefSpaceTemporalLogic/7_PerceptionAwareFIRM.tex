\documentclass[conference]{IEEEtran}
\usepackage{standalone}
\usepackage{times}
\usepackage{float}
\usepackage{biblatex}
\addbibresource{7_refs.bib}


\input{TemplateFiles/def.tex}
\input{TemplateFiles/inc}

% numbers option provides compact numerical references in the text. 

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

% Table caption wrangling
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatletter
\patchcmd{\@makecaption}
  {\\}
  {.\ }
  {}
  {}
\makeatother

\newcommand{\Ali}[1]{{\color{green} Ali: #1}}
\newcommand{\cristi}[1]{{\color{orange} Cristi: #1}}
\newcommand{\rohan}[1]{{\color{blue} Rohan: #1}}
\allowdisplaybreaks[1]

\begin{document}

% paper title
%\title{\large From Mission Specification to Safe Control Logic Under Uncertainty:\\ Application to Mars Copter-Rover Navigation-Coordination}

\title{\huge Vision Aware FIRM}

\author{Rohan, Ali, Kamak, Cristi, Petter, Sofie, Richard, Murray, Aaron Ames}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\section{Related work}
\cite{achtelik2014motion}

\cite{costante2018exploiting}

\cite{perception-aware-planning} 

\cite{Perception-Aware-Multiobjective-Search}

\cite{MapQualityEvaluation}

\section{Motivation}
Talk about degenerate configurations with figures.

\section{Sensor Model}

Let $P \in \mathbb{R}^{3}$ represent a 3-d point, $p \in \mathbb{R}^2$ represent represent a point in the image space and $\pi_\theta:\mathbb{R}^3\to\mathbb{R}^2$ be the camera projection model assuming that the intrinsic camera parameters are known and $\theta$ represents the pose of the camera in the world frame.

The equation below projects a point $P$ in the world frame to image space of $k^{th}$ camera frame:
\begin{equation} p=\pi_\theta(^{w}P) \end{equation}
We can retrieve the 3-d point in the world frame from a point in the image space and known depth, using: 
\begin{equation} ^{w}P=\pi^{-1}_\theta(p,d) \end{equation}

The sensor model is obtained by using the following equation, where $\Sigma_o$ is covariance of the image noise (estimation of location of the feature points).
\begin{equation}
     z_p = h(\theta,v)=
     \pi_\theta(\pi^{-1}_{\theta_{kf}}(p,d)) + v,~~~ v\sim \mathcal{N}(0,\Sigma_{o})
\end{equation}


The pose of the camera $\theta$ can be estimated by minimizing the re-projection error, as shown in the equation below. Where, $\theta_{kf}$ represents the pose of the keyframe and $p_z$ represents the measure location of the feature point in the image space.
\begin{equation}
     \theta = \underset{\theta}{\arg\min}~||z_p - \pi_\theta(\pi^{-1}_{\theta_{kf}}(p,d))||^2_{\Sigma_o}
\end{equation}

Information $\mathcal{I}$ can be found using the following equation:
\begin{equation}
     \mathcal{I} = J^{T}\Sigma^{-1}_{o}J
\end{equation}
for
\begin{equation}
     J = \frac{dh}{d\theta}(?)
\end{equation}

Advantages:
\begin{itemize}
    \item Computationally more efficient than direct or homography based approaches.
    \item Does not require homography assumptions like planar scenes, etc.
\end{itemize}

Challenges:
\begin{itemize}
    \item Efficiently finding all the feature points in the field for a give camera pose $\theta$.
    \item Acquiring set of keyframes from an elevation map of the environment.
\end{itemize}

\printbibliography

\end{document}
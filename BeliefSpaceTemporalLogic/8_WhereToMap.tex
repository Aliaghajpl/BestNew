\documentclass[conference]{IEEEtran}
\usepackage{standalone}
\usepackage{times}
\usepackage{float}

\input{TemplateFiles/def.tex}
\input{TemplateFiles/inc}

% numbers option provides compact numerical references in the text. 

% Table caption wrangling
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatletter
\patchcmd{\@makecaption}
  {\\}
  {.\ }
  {}
  {}
\makeatother

\newcommand{\Ali}[1]{{\color{green} Ali: #1}}
\newcommand{\cristi}[1]{{\color{orange} Cristi: #1}}
%\newcommand{\rohan}[1]{{\color{blue} Rohan: #1}}
\allowdisplaybreaks[1]

\begin{document}

% paper title
\title{\huge Mobile Eye in the Sky: Copter-Rover Coordination for Space Exploration}

%\author{Takahiro Sasaki, Kyohei Otsu, Ali-akbar Agha-mohammadi, Kamak Ebadi}

\maketitle

\begin{abstract}
In this paper, we consider the problem of planetary exploration with a two-agent team composed of a Mars rover and a Mars helicopter. Images by a helicopter can help estimation or navigation of a rover. In such a mission, how a helicopter learns where to map for a rover is of great interest in a two-agent team. 
This paper answers this where to map by a helicopter while minimizing uncertainty of images from a rover’s camera.
\end{abstract}

\IEEEpeerreviewmaketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%\subsection*{Related Work}
%\subsection*{Contributions}

\section{Problem Description}
This section gives a formal definition of the problem we are going to address. 

\pr{Problem}
This paper automates the process of where to map by a copter in order to minimize the covariance or the localization belief of a rover while considering the copter’s body path. Figure~\ref{fig:8_1} shows a belief propagation of a rover and Fig.~\ref{fig:8_2} illastrates an image of where to map by a copter.

\pr{Rover and Helicopter Description}
The rover and helicopter configurations are denoted by $x^r$ and $x^c$, respectively. The extended state $x=(x^r,~x^c)$ includes both.

\pr{Rover and Helicopter Measurements}
Let $z^r_k$ and $z^c_k$ the measurements (images) by the rover and the helicopter at the $k$-th time step, respectively. Sequences of observations are defined as $z^r_{0:k}=\{z^r_0,z^r_1,\cdots,z^r_k\}$,~$z^c_{0:l}=\{z^c_0,z^c_1,\cdots,z^c_l\}$ for $0 \leq l \leq k$. A copter takes images only at the $l$-th step ($l \in k$), since a copter has battery limitation.

\pr{Map Belief}
We denote the map of the environment by $m$ and assume the environment is stationary. The knowledge we have about the environment is obtained via noisy sensors. Therefore, the best we can have is a probabilistic representation of $m$, i.e., the map belief
\begin{align}
    b^m=p(m|z, x)
\end{align}

\pr{Measurement Model}
The measurement model describes the probability distribution over all possible observations given the robots' state $x$ and the environment map $m$.
\begin{align}
    p(z|m, x)
\end{align}



\begin{figure}[h]
		\centering
		\includegraphics[width=1.0\columnwidth]{figs/8_1.png}
		\caption{Rover path.}
		\label{fig:8_1}
		\centering
		\includegraphics[width=1.0\columnwidth]{figs/8_2.png}
		\caption{Where to map.}
		\label{fig:8_2}
\end{figure}


\section{Mapping}

\pr{Map Representation}
We consider maps from both rover and copter and use a surf feature mapping. Vectors or matrices $m^r$ and $m^c$ denote maps by a rover and a copter, respectively. Finally, a map can be obtained by combining $m^r$ and $m^c$.

\pr{Map Update}
The map of a rover and a copter can be updated as follows:
\begin{align}
    m^r_{k+1}&=\tau^r_m (m_k, u^r_k, x^r_{k+1}, z^r_{k+1})\\
    m^c_{k+1}&=\tau^c_m (m_k, u^c_k, x^c_{k+1}, z^c_{k+1})
\end{align}
where
\begin{align}
    z^r_{k+1} &\sim p(\cdot |x^r_{k+1})\\
    x^r_{k+1} &\sim p(\cdot |x^r_k, u^r_k, w^r_k)\\
    z^c_{k+1} &\sim p(\cdot |x^c_{k+1})\\
    x^c_{k+1} &\sim p(\cdot |x^c_k, u^c_k, w^c_k).
\end{align}
Therefore, using these $m^r_{k+1}$ and $m^c_{k+1}$, the map at the $k$-th time step can be obtained by

\begin{eqnarray}
m_{k+1} =
  \begin{cases}
    \tau_m (m^r_{k+1}, m^c_{k+1}) & ($k+1 \in l$)\\
    m^r_{k+1} & ($k+1 \not\in l$)
  \end{cases}
\end{eqnarray}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rover Localization}
This section considers belief update for a rover localization. Belief update of a map is given by
\begin{align}
    b^{m}_k=p(m_k | z^r_{0:k}, x^r_{0:k}, z^c_{0:l}, x^c_{0:l}).
\end{align}
Belief update of a rover localization is given by
\begin{align}
    b^r_{k+1}=\tau^r (b^r_k, u^r_k, z^r_{k+1}; m_{k+1}).
\end{align}

\section{Planning: copter where to map}
\subsection{Planning Objective}
%Covariance (Exploitation)\\

We find a helicopter position that minimizes the following cost function:
\begin{align}
    &x^{c*}=\arg\min_{x^c \in X^c} \sum^T_{k=1} P^{r+}_k\\
     &{\rm s.t.} \hspace{5mm} D_{\min}(x^c_{alt}) \leq D^c_k \leq D_{\max}(x^c_{alt})\label{eq:st1}\\
    &\hspace{10mm} L^c \leq L^c_{\max}(T^c_{search})\label{eq:st2}
\end{align}
where
\begin{align}
P^{r+}_k&={\rm Cov}(x^{r+}_k) = {\rm Cov}(x^{r}_k | z_{0:k})\\
&={\rm VO}(z_1,~z_2,\cdots,~z_k)\simeq {\rm VO}(z_k).
\end{align}
Note that Cov($\cdot$) and VO($\cdot$) denote the covariance and the visual odometry, respectively.

\subsection{Constraints}

\pr{Resolution constraint}
A resolution of an image from a copter dependes on a camera performance and a altitude of a coptrer $x^c_{alt}$.
%\begin{align}
%    \zeta=k x^c_{alt}
%\end{align}

\pr{Field-of-view constraint}
Equation~\eqref{eq:st1} represents the constraint that the area of the image depends on the altitude of the copter.
A field-of-view (FOV) $D^c$ is determined by an altitude of a copter.
\begin{align}
    D_{\min}(x^c_{alt}) \leq D^c_k \leq D_{\max}(x^c_{alt})
\end{align}

\pr{Flight time constraint}
Equation~\eqref{eq:st2} represents the constraint that the searching range $L^c$ depends on the flight time $T^c_{search}$ coming from the battery limitation of the copter.
\begin{align}
    L^c \leq L^c_{\max}(T^c_{search})
\end{align}

\pr{Flight interval constraint}
``{\it When to map}'' is also an interesting problem. This constraint determines $l$ in the time step $k$.

\section{Planning: copter body path planning}
\subsection{Planning Objective}
Using the cost $C(b_k, u_k)$,
\begin{align}
    &u^{c*}_{0:k}=\arg\min_{x^c \in X^c} \mathbb{E} \sum^T_{k=0} C(b^r_k, u_k; m_k (u^c_{0:k}))\\
    &{\rm s.t.} \hspace{5mm} ||u^c_{0:k}||< \delta\label{eq:st10}\\
    &\hspace{10mm} x^r_T \in goal^r\label{eq:st20}
\end{align}

%Observation $z_k$ is given by
%\begin{align}
%z_k=x^r_k-L^m_k(x^c_k)
%\end{align}

\subsection{Constraints}

\pr{Input constraint}
Equation~\eqref{eq:st10} represents the constraint that the limitation of the copter control input as follow:
\begin{align}
    ||u^c_{0:k}||< \delta
\end{align}

\pr{Position constraint}
Equation~\eqref{eq:st20} represents the constraint that the rover need to reach the goal at the final step $T$.
\begin{align}
    x^r_T \in goal^r
\end{align}

\section{Search Method: copter where to map}
%RRT
\pr{Grid the Search Space}
First of all, we grid the searching space to determine where to map and the copter's path as in Fig.~\ref{fig:8_3}.\\
\begin{figure}[h]
		\centering
		\includegraphics[width=1.0\columnwidth]{figs/8_3.png}
		\caption{Searching space grid.}
		\label{fig:8_3}
\end{figure}


%\subsection{Planning Objective~2: Coverage (Exploration)}

Our method investigates unseen or uncertainty areas by helicopter.
\subsection*{Coverage (Exploration)}
\begin{itemize}
    \item Occluded area (using Viewshed analysis)
    \item Beyond rover’s perception horizon
\end{itemize}

Using Viewshed analysis with $(x^r,~m)$, coverage can be obtained as shown in Fig.~\ref{fig:vs}.
\begin{figure}[h]
		\centering
		\includegraphics[width=1.0\columnwidth]{figs/vs.png}
		\caption{Viewshed analysis (example).}
		\label{fig:vs}
\end{figure}



\pr{Perception Performance Prediction}
aaa\\
RRT?

\section{Search Method: copter body path planning}
RRT?

\section{Simulation Results}

\section{Conclusion}

\end{document}


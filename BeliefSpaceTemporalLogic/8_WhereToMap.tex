\documentclass[conference]{IEEEtran}
\usepackage{standalone}
\usepackage{times}
\usepackage{float}

\input{TemplateFiles/def.tex}
\input{TemplateFiles/inc}

% numbers option provides compact numerical references in the text. 

% Table caption wrangling
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatletter
\patchcmd{\@makecaption}
  {\\}
  {.\ }
  {}
  {}
\makeatother

\newcommand{\Ali}[1]{{\color{green} Ali: #1}}
\newcommand{\cristi}[1]{{\color{orange} Cristi: #1}}
%\newcommand{\rohan}[1]{{\color{blue} Rohan: #1}}
\allowdisplaybreaks[1]

\begin{document}

% paper title
\title{\huge Mobile Eye in the Sky: Copter-Rover Coordination for Space Exploration}

%\author{Takahiro Sasaki, Kyohei Otsu, Ali-akbar Agha-mohammadi, Kamak Ebadi}

\maketitle

\begin{abstract}
In this paper, we consider the problem of planetary exploration with a two-agent team composed of a Mars rover and copter. Images by a copter can help estimation or navigation of a rover. In such a mission, how a copter learns where to map for a rover is of great interest in a two-agent team. 
This paper answers this where to map by a copter while minimizing uncertainty of a map.
\end{abstract}

\IEEEpeerreviewmaketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\subsection*{Motivations}
About Mars exploration---------

About Mars copter exploration or multi-agent Mars exploration and PDF project---------

This study considers a two-agent team composed of a rover and a copter. Figures~\ref{fig:8_1} and \ref{fig:8_2} show an example of comparison between rover and two-agent team cases. An expected effectiveness of a two-agent team is also shown. Assuming that a rover has only low-resolution images from Mars orbiters at the start, a rover executes path planning using those images.

Figure~\ref{fig:8_1} shows the case that the risk region is the obstacle which the rover cannot go through and have to avoid. The actual image and the satellite image are shown as in Fig.~\ref{fig:8_1}~(a) and (b), respectively. In the satellite image, the obstacle is described as the risk region since the satellite image is low-resolution and has uncertainty. The black lines and the red lines denote the rover’s and copter’s paths, respectively. Figure~\ref{fig:8_1}~(c) shows an example of rover exploration without a copter. In this case, once a rover is close to the risk region, it proves to have obstacles by a rover’s camera. On the other hand, Fig.~\ref{fig:8_1}~(d) shows an example of rover exploration with a copter. In this case, a rover can learn that the risk region includes obstacles in advance by copter’s camera images. Therefore, a rover can re-execute the optimal path planning before starting the exploration by a copter.

Figure~\ref{fig:8_2} shows the case that the risk region is the feature-poor areas which the rover can go through but the accurate estimation of a rover is difficult to realize. The green and black solid lines show the initial and updated rover’s paths, respectively, and the red dashed line shows the copter’s path for updating the initial rover’s path. The yellow and blue ellipses areas are covariance along each path. In the case of a two-agent team, a rover can learn that the risk region includes feature-poor areas in advance by copter’s camera images. Therefore, a rover can re-execute the optimal path planning by a copter, again.
Figure~\ref{fig:8_4} shows the relationship of rover/copter cooperation.

\subsection*{Related Work}
Multi-agent on the Earth-----------

Multi-agent on Mars-----.

\subsection*{Contributions}
-------------

\begin{figure}[h]
		\centering
		\includegraphics[width=0.9\columnwidth]{figs/8_1.png}
		\caption{Rover/copter paths with an obstacle.}
		\label{fig:8_1}
		\vspace{5mm}
		\centering
		\includegraphics[width=0.6\columnwidth]{figs/8_2.png}
		\caption{Rover/copter paths with feature-poor areas.}
		\label{fig:8_2}
		\vspace{5mm}
		\centering
		\includegraphics[width=0.6\columnwidth]{figs/8_4.png}
		\caption{Relationship of rover/copter cooperation.}
		\label{fig:8_4}
\end{figure}

\clearpage
\section{Problem Description}
This section gives a formal definition of the problem we are going to address. 

\pr{Problem}
A two-agent team composed of a Mars rover and copter is considered. This paper automates the process of where to map by a copter in order to minimize the covariance of a rover while considering the copter’s body path.
The copter is assumed that
\begin{itemize}
    \item Camera’s direction 
    \item Three flights per sol
    \item Two minutes per flight
\end{itemize}
and a rover is assumed that
\begin{itemize}
    \item Rover stays when the copter flies
\end{itemize}
and a two-agent have to keep distance for safety.
 
 \begin{figure}[b]
		\centering
		\includegraphics[width=0.8\columnwidth]{figs/8_3.png}
		\caption{Definition of the rover/copter time step.}
		\label{fig:8_3}
\end{figure}

\pr{Rover and copter Description}
The rover and copter configurations are denoted by $x^r$ and $x^c$, respectively. The extended state $x=(x^r,~x^c)$ includes both.
The rover and copter's time steps are described as in Fig.~\ref{fig:8_3} and the rover and copter's paths are denoted by $x^r_{0:i}$ and $x^c_{0:k}$, respectively.

\pr{Rover and copter Measurements}
Let $z^r_i$ and $z^c_k$ the measurements (images) by the rover and the copter at the $k$-th and $i$-th time steps, respectively. Sequences of observations are defined as
\begin{align}
    z^r_{0:i}&=\{z^r_0,z^r_1,\cdots,z^r_i\}\\
    z^c_{0:k}&=\{z^c_{0},z^c_{1},\cdots,z^c_{k}\}
\end{align}
where $0 \leq i \leq T$ and $0 \leq k \leq N$. Note that $T$ and $N$ denote the final states of the rover and the copter.

\pr{Map Description}
The map coming from the satellite's images is denoted by $m^s$. An updated map is given by $m$. 

\pr{Map Belief}
We denote the map of the environment by $m$ and assume the environment is stationary. The knowledge we have about the environment is obtained via noisy sensors. Therefore, the best we can have is a probabilistic representation of $m$, i.e., the map belief
\begin{align}
    b^m=p(m|z, x)
\end{align}

\pr{Measurement Model}
The measurement model describes the probability distribution over all possible observations given the robots' state $x$ and the environment map $m$.
\begin{align}
    p(z|m, x)
\end{align}

\pr{Proposed Scenario}
The proposed scenario is given in Algorithm~\ref{alg:1}.

\begin{algorithm}[!ht]
\caption{Scenario for rover/copter exploration}
\label{alg:1}
\DontPrintSemicolon
%\BlankLine
Making an initial map $m=m^s$ using the satellite's image\;
%%%%
Choose an initial rover's path $x^r_{0:i}$ by the rover path planner (Section~\ref{sec:r})\;
%%%%
\For{$1$ \KwTo $n$ (the number of copter's flights)}{
    Choose the rover/copter paths $x^r_{0:i}$ and $x^c_{0:k}$ by the rover/copter path planner (Section~\ref{sec:r/c})\;
    Update the map $m$ using the copter's images $z^{c_j}_{0:k}$\;
    Keep a rover tracking until next viewpoint or goal while propagating the rover covariance and updating\;
}
\Return distance and covariance of the rover at the goal $(d_{0,T}, P_T)$
\end{algorithm}


\section{Mapping}

\pr{Map Representation}
%We consider maps from both rover and copter and use a surf feature mapping. Vectors or matrices $m^r$ and $m^c$ denote maps by a rover and a copter, respectively. Finally, a map can be obtained by combining $m^r$ and $m^c$.

\pr{Map Update}
The map of the rover and a the copter can be updated as follows:
\begin{align}
m_{i+1} = \tau^m (m_{i}, z^{c_j}_{0:N}, z^r_{0:i+1})
\end{align}


%\begin{align}
%    m^r_{k+1}&=\tau^r_m (m_k, x^r_{k+1}, z^r_{k+1})\\
%    m^c_{k+1}&=\tau^c_m (m_k, x^c_{k+1}, z^c_{k+1})
%\end{align}
%where
%\begin{align}
%    z^r_{k+1} &\sim p(\cdot |x^r_{k+1})\\
%    x^r_{k+1} &\sim p(\cdot |x^r_k, u^r_k, w^r_k)\\
%    z^c_{k+1} &\sim p(\cdot |x^c_{k+1})\\
%    x^c_{k+1} &\sim p(\cdot |x^c_k, u^c_k, w^c_k).
%\end{align}
%Therefore, using these $m^r_{k+1}$ and $m^c_{k+1}$, the map at the $k$-th time step can be obtained by

%\begin{eqnarray}
%m_{k+1} =
%  \begin{cases}
%    \tau_m (m^r_{k+1}, m^c_{k+1}) & (k+1 = l)\\
%    m^r_{k+1} & (k+1 \neq l)
%  \end{cases}
%\end{eqnarray}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rover Localization}

%\subsection{Error Propagation Model}
%\subsection{Linearized Error Propagation Model}

This section considers the propagation of the rover's covariance. We use the linearized error propagation model in Ref\cite{IEEE:Inoue}.
The state covariance matrix propagates as follows:
\begin{align}
    P_j=A^{ij} P_i {A^{ij}}^T + B^{ij} \Delta \Sigma^{ij} {B^{ij}}^T
    \label{eq:Pj}
\end{align}
where the level of error in the measurements in state increments $\Sigma^{ij}$ is
\begin{align}
    \Sigma^{ij}=d_{i,j}\cdot \Delta \sigma.
\end{align}
Note that $A^{ij}$ and $B^{ij} \Delta \Sigma^{ij} {B^{ij}}^T$ are specific to a given edge and independent from the state in nodes $i$ and $j$. $\Delta \sigma$ denotes the accumulation rate of the covariance and depends on the sensor used for odometry, e.g., VO or WO and uncertainty of the current map.


%This section considers belief update for a rover localization. Belief update of a map is given by
%\begin{eqnarray}
%    b^{m}_k=
%    \begin{cases}
%    p(m_k | z^r_{0:k}, x^r_{0:k}, z^c_{k,1:j}, x^c_{k,1:j}) & (k=l)\\
%    p(m_k | z^r_{0:k}, x^r_{0:k}) & (k \neq l)
%    \end{cases}
%\end{eqnarray}
%Belief update of a rover localization is given by
%\begin{align}
%    b^r_{k+1}=\tau^r (b^r_k, b^m_k, u^r_k, z^r_{k+1}; m_{k+1}).
%\end{align}

\section{Rover path planning}\label{sec:r}
\subsection{Planning Objective}
We find a rover path that minimizes the following cost function:
\begin{align}
  &x^{r*}_{0:i}={\rm arg} \min_{x^r_{0:i}} J\\
  &\hspace{3mm}{\rm s.t.}\hspace{3mm}x^r_{T} \in X^r_{goal}\label{eq:J1c1}\\
\end{align}
where
\begin{align}
    J=(1-\alpha) \sum^T_{i=0}  {\rm tr}(P^r_i) +\alpha \sum^T_{i=1} d_{i,i+1}
\end{align}
Note that $P^r_i$ is the covariance matrix of the rover at the $i$-th step and ${\rm tr}(*)$ is the trace of $*$. The covariance matrix is calculated in Eq.~\eqref{eq:Pj}. The constraint in \eqref{eq:J1c1} represents the rover position at the final step exists within the goal.

\subsection{Search Method}
RRT$^*$

\section{Rover/copter path planning}\label{sec:r/c}
\subsection{Planning Objective}
We find a copter path that minimizes the following cost function:
\begin{align}
  &\{ x^{r*}_{0:i}, x^{c*}_{0:k}\}={\rm arg} \min_{x^r_{0:i},x^c_{0:k}} J\\
  &\hspace{3mm}{\rm s.t.}\hspace{3mm}x^r_{T} \in X^r_{goal}\label{eq:J2c1}\\
  &\hspace{10.5mm} h_{\min} \leq z^c \leq h_{\max}\label{eq:J2c2}\\
  &\hspace{10.5mm} 0 \leq N \leq k_{\max}\label{eq:J2c3}\\
  %&\hspace{10.5mm} d_{\min} \leq ||x^c_t-x^r_t|| \leq d_{\max},~ ~^\forall t \in [0,k_{max}]\label{eq:J2c4}\\
  &\hspace{10.5mm} x^c_N \not\in \{ x^{r*}_{0:i}+\Delta \}\label{eq:J2c5}
\end{align}
where
\begin{align}
    J=(1-\alpha) \sum^T_{i=0}  {\rm tr}(P^r_i) +\alpha \sum^T_{i=1} d_{i,i+1}
\end{align}

\subsection{Constraints}

\pr{Final Rover Position Constraint}
The first constraint in \eqref{eq:J2c1} represents the rover position at the final step exists within the goal as well as the constraint in \eqref{eq:J1c1}.

\pr{Copter Altitude Constraint}
The constraint in Eq.~\eqref{eq:J2c2} comes from both copter's field of view (FOV) constraint and camera's resolution constraint.
The FOV area $D^c$ and the resolution measurement $R^c$ of the copter's camera depend on the altitude of the copter and represent by
\begin{align}
    D^c_k (z_k^c)&=4 (z_k^c \tan \theta)^2\\
    R^c_k (z_k^c)&=\cfrac{\alpha}{z_k^c} ,
\end{align}
Note that $\theta$ is the view angle of the camera and $\alpha$ is the positive scalar. 

\pr{Copter Time Constraint}
Equation~\eqref{eq:J2c3} shows the constraint of the copter's flight time. $k_{\max}$ is the maximum time step of the copter coming from the limitation of the copter's battery. Then, the final step of the copter $N$ needs to be less than $k_{\max}$.

%\pr{Rover/copter Position Constraint}
%\label{eq:J2c4}

\pr{Copter Position Constraint}
Final constraint in the planning framework comes from the copter's position at the final step in Eq.~\eqref{eq:J2c5}. Parameter $\Delta$ is the distance margin between rover and copter for their safety. This constraint can be illustrated as in Fig.~\ref{fig:8_5}.

\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\columnwidth]{figs/8_5.png}
		\caption{Illustration of the copter position constraint at the final step.}
		\label{fig:8_5}
\end{figure}

%\pr{Resolution constraint}
%A resolution of an image from a copter depends on a camera performance and a altitude of a copter $x^c_{alt}$.
%\begin{align}
%    \zeta=k x^c_{alt}
%\end{align}

%\pr{Field-of-view constraint}
%Equation~\eqref{eq:st1} represents the constraint that the area of the image depends on the altitude of the copter.
%A field-of-view (FOV) $D^c$ is determined by an altitude of a copter.
%\begin{align}
%    D^c_{\min}(x^c_{alt}) \leq D^c_k \leq D^c_{\max}(x^c_{alt})
%\end{align}

%\pr{Flight time constraint}
%Equation~\eqref{eq:st2} represents the constraint that the searching range $L^c$ depends on the flight time $t^c_{search}$ coming from the battery limitation of the copter.
%\begin{align}
%    \sum^{n}_{i=0} L^c_{k,i} \leq L^c_{\max}(t^c_{search})
%\end{align}
%where $n$ is the number of the areas for searching by the copter.

%\pr{Flight interval constraint}
%``{\it When to map}'' is also an interesting problem.

\subsection{Search Method}
RRT$^*$

%\section{Planning: copter body path planning}
%\subsection{Planning Objective}
%Using the cost $C(b_k, u_k)$,
%\begin{align}
%    &u^{c*}_{0:k}=\arg\min_{x^c \in X^c} \mathbb{E} \sum^T_{k=0} C(b^r_k, u_k; m_k (u^c_{0:k}))\\
%    &{\rm s.t.} \hspace{5mm} ||u^c_{0:k}||< \delta\label{eq:st10}\\
%    &\hspace{10mm} x^r_T \in goal^r\label{eq:st20}
%\end{align}

%\subsection{Constraints}
%\pr{Input constraint}
%Equation~\eqref{eq:st10} represents the constraint that the limitation of the copter control input as follow:
%\begin{align}
%    ||u^c_{0:k}||< \delta
%\end{align}

%\pr{Position constraint}
%Equation~\eqref{eq:st20} represents the constraint that the rover need to reach the goal at the final step $T$.
%\begin{align}
 %   x^r_T \in goal^r
%\end{align}

%\subsection{Search Method}


\section{Simulation Results}

\section{Conclusion}

\begin{thebibliography}{1}

\bibitem{IEEE:Kyon}
K. Otsu, A. Agha-mohammadi, and M. Paton, ``Where to look? Predictive Perception with Applications to Planetary Exploration,'' in {\it IEEE Robotics and Automation Letters}, vol.xx, no.xx, pp.xx-xx, 201x.

\bibitem{IEEE:Inoue}
H. Inoue, M. Ono, S. Tamaki and S. Adachi, ``Active Localization for Planetary Rovers,'' in {\it IEEE Aerospace Conference 2016}, Big Sky, MT, USA, 2016.

\end{thebibliography}

\end{document}


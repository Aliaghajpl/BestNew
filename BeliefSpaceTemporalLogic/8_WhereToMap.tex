\documentclass[conference]{IEEEtran}
\usepackage{standalone}
\usepackage{times}
\usepackage{float}

\input{TemplateFiles/def.tex}
\input{TemplateFiles/inc}

% numbers option provides compact numerical references in the text. 

% Table caption wrangling
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatletter
\patchcmd{\@makecaption}
  {\\}
  {.\ }
  {}
  {}
\makeatother

\newcommand{\Ali}[1]{{\color{green} Ali: #1}}
\newcommand{\cristi}[1]{{\color{orange} Cristi: #1}}
%\newcommand{\rohan}[1]{{\color{blue} Rohan: #1}}
\allowdisplaybreaks[1]

\begin{document}

% paper title
\title{\huge Mobile Eye in the Sky: Copter-Rover Coordination for Space Exploration}

%\author{Takahiro Sasaki, Kyohei Otsu, Ali-akbar Agha-mohammadi, Kamak Ebadi}

\maketitle

\begin{abstract}
In this paper, we consider the problem of planetary exploration with a two-agent team composed of a Mars rover and a Mars helicopter. Images by a helicopter can help estimation or navigation of a rover. In such a mission, how a helicopter learns where to map for a rover is of great interest in a two-agent team. 
This paper answers this where to map by a helicopter while minimizing uncertainty of images from a rover’s camera.
\end{abstract}

\IEEEpeerreviewmaketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%\subsection*{Related Work}
%\subsection*{Contributions}

\section{Problem Description}
This section gives a formal definition of the problem we are going to address. 

\pr{Rover and Helicopter Description}
The rover and helicopter configurations are denoted by $x^r$ and $x^c$, respectively. The extended state $x=(x^r,~x^c)$ includes both.

\pr{Rover and Helicopter Measurements}
Let $z^r_k$ and $z^c_k$ the measurements (images) by the rover and the helicopter at the $k$-th time step, respectively. Sequences of observations are defined as $z^r_{i:j}=\{z^r_i,z^r_{i+1},\cdots,z^r_j\}$,~$z^c_{i:j}=\{z^c_i,z^c_{i+1},\cdots,z^c_j\}$ for $j>i$.

\pr{Map Belief}
We denote the map of the environment by $m$ and assume the environment is stationary. The knowledge we have about the environment is obtained via noisy sensors. Therefore, the best we can have is a probabilistic representation of $m$, i.e., the map belief
\begin{align}
    b^{m}_k&=p(m | z^r_{0:k}, x^r_{0:k})
\end{align}
or
\begin{align}
    b^{m}_k&=p(m | z^r_{0:k}, x^r_{0:k}, z^c, x^c)
\end{align}
%one by one??

\pr{Measurement Model}
The measurement model describes the probability distribution over all possible observations given the robots' state $x$ and the environment map $m$.
\begin{align}
    p(z|m, x)
\end{align}

\pr{Problem}

\section{Mapping}

\pr{Map Representation}
%Confidence-rich Grid Mapping
Surf features
\begin{align}
    m=
\end{align}

\pr{Map Evolution}

\begin{align}
    m_{k+1}=\tau^m (m_k, u^c, z^c_{k+1})
\end{align}
where
\begin{align}
    z^c_{k+1} &\sim p(\cdot |x^c_{k+1})\\
    x^c_{k+1} &\sim p(\cdot |x^c_k, u^c_k, w^c_k)
\end{align}


%\pr{Map Update}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rover Localization}

\begin{align}
    b^r_{k+1}=\tau^r (b^r_k, u^r_k, z^r_{k+1}; m)
\end{align}


\section{Planning}
\subsection{Planning Objective}

\begin{align}
    c(b_k, u_k)=
\end{align}

Covariance (Exploitation)\\


We find a helicopter position that minimizes the following cost function:
\begin{align}
    x^{c*}=\arg\min_{x^c \in X^c} \sum^T_{k=1} P^{r+}_k
\end{align}
where
\begin{align}
P^{r+}_k&={\rm Cov}(x^{r+}_k) = Cov(x^{r}_k | z_{0:k})\\
&={\rm VO}(z_1,~z_2,\cdots,~z_k)\simeq {\rm VO}(z_k).
\end{align}
Using the cost $C(b_k, u_k)$,
\begin{align}
    &u^{c*}_{0:k}=\arg\min_{x^c \in X^c} \mathbb{E} \sum^T_{k=0} C(b^r_k, u_k; m_k (u^c_{0:k}))\\
    &{\rm s.t.} \hspace{5mm} ||u^c_{0:k}||< \delta\\
    &\hspace{10mm} x^r_T \in goal^r
\end{align}






%Observation $z_k$ is given by
%\begin{align}
%z_k=x^r_k-L^m_k(x^c_k)
%\end{align}


%\subsection{Planning Objective~2: Coverage (Exploration)}
%We investigate unseen areas by helicopter.
%\begin{itemize}
%    \item Occluded area (using Viewshed analysis)
%    \item Beyond rover’s perception horizon
%\end{itemize}
%Using Viewshed analysis with $(x^r,~m)$, coverage can be obtained.
%\begin{figure}[b]
%		\centering
%		\includegraphics[width=1.0\columnwidth]{figs/vs.png}
%		\caption{Viewshed analysis.}
%		\label{fig:vs}
%\end{figure}

\subsection{Planning Constraints}

\subsubsection{Field-of-view constraint}
~
\subsubsection{Resolution constraint}
~
\subsubsection{Flight time constraint}
~
\subsubsection{Flight interval constraint}
~

\section{Search Method}
%RRT
\pr{Grid the Search Space}

\section{Simulation Results}

\section{Conclusion}

\end{document}

